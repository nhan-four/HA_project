"""
Batch Processor - X·ª≠ l√Ω d·ªØ li·ªáu theo batch ƒë·ªÉ tr√°nh tr√†n RAM

H·ªó tr·ª£:
- X·ª≠ l√Ω dataset l·ªõn (h√†ng tri·ªáu d√≤ng)
- Gi·ªõi h·∫°n RAM s·ª≠ d·ª•ng
- Generator-based processing
- Memory monitoring
"""

import numpy as np
import psutil
import gc
from typing import Generator, Tuple, Optional, Callable, Iterator
from dataclasses import dataclass

from .logger import get_logger


@dataclass
class MemoryConfig:
    """
    C·∫•u h√¨nh gi·ªõi h·∫°n b·ªô nh·ªõ.
    
    Attributes:
        max_memory_gb: RAM t·ªëi ƒëa ƒë∆∞·ª£c s·ª≠ d·ª•ng (GB)
        batch_size: S·ªë samples m·ªói batch (t·ª± ƒë·ªông t√≠nh n·∫øu None)
        reserve_memory_gb: RAM d·ª± tr·ªØ cho h·ªá th·ªëng (GB)
    """
    max_memory_gb: float = 4.0
    batch_size: Optional[int] = None
    reserve_memory_gb: float = 1.0
    
    def __post_init__(self):
        if self.max_memory_gb <= 0:
            raise ValueError("max_memory_gb ph·∫£i > 0")
        if self.reserve_memory_gb < 0:
            raise ValueError("reserve_memory_gb ph·∫£i >= 0")


class MemoryManager:
    """
    Qu·∫£n l√Ω b·ªô nh·ªõ RAM.
    
    Theo d√µi v√† ki·ªÉm so√°t vi·ªác s·ª≠ d·ª•ng RAM ƒë·ªÉ tr√°nh tr√†n b·ªô nh·ªõ.
    """
    
    def __init__(self, config: MemoryConfig):
        self.config = config
        self.logger = get_logger("MemoryManager", level="WARNING", log_to_file=False)
        
        # T√≠nh to√°n gi·ªõi h·∫°n th·ª±c t·∫ø
        total_ram_gb = psutil.virtual_memory().total / (1024**3)
        self.max_bytes = min(
            config.max_memory_gb * (1024**3),
            (total_ram_gb - config.reserve_memory_gb) * (1024**3)
        )
        
        self.logger.info(f"Memory limit: {self.max_bytes / (1024**3):.2f} GB")
    
    def get_current_usage(self) -> float:
        """Tr·∫£ v·ªÅ RAM ƒëang s·ª≠ d·ª•ng (bytes)."""
        process = psutil.Process()
        return process.memory_info().rss
    
    def get_available_memory(self) -> float:
        """Tr·∫£ v·ªÅ RAM c√≤n kh·∫£ d·ª•ng (bytes)."""
        return self.max_bytes - self.get_current_usage()
    
    def estimate_array_size(self, shape: Tuple[int, ...], dtype=np.float64) -> int:
        """∆Ø·ªõc t√≠nh k√≠ch th∆∞·ªõc array (bytes)."""
        return np.prod(shape) * np.dtype(dtype).itemsize
    
    def calculate_optimal_batch_size(
        self,
        n_features: int,
        dtype=np.float64,
        safety_factor: float = 0.7
    ) -> int:
        """
        T√≠nh batch size t·ªëi ∆∞u d·ª±a tr√™n RAM kh·∫£ d·ª•ng.
        
        Args:
            n_features: S·ªë features
            dtype: Ki·ªÉu d·ªØ li·ªáu
            safety_factor: H·ªá s·ªë an to√†n (0-1)
        
        Returns:
            int: Batch size t·ªëi ∆∞u
        """
        if self.config.batch_size is not None:
            return self.config.batch_size
        
        available = self.get_available_memory() * safety_factor
        bytes_per_sample = n_features * np.dtype(dtype).itemsize
        
        # C·∫ßn th√™m b·ªô nh·ªõ cho c√°c bi·∫øn trung gian (~3x)
        batch_size = int(available / (bytes_per_sample * 3))
        
        # Gi·ªõi h·∫°n trong kho·∫£ng h·ª£p l√Ω
        batch_size = max(100, min(batch_size, 100000))
        
        return batch_size
    
    def force_garbage_collection(self):
        """Bu·ªôc thu gom r√°c ƒë·ªÉ gi·∫£i ph√≥ng RAM."""
        gc.collect()
    
    def check_memory_ok(self, required_bytes: int) -> bool:
        """Ki·ªÉm tra c√≥ ƒë·ªß RAM kh√¥ng."""
        return self.get_available_memory() >= required_bytes


class BatchProcessor:
    """
    X·ª≠ l√Ω d·ªØ li·ªáu theo batch ƒë·ªÉ tr√°nh tr√†n RAM.
    
    Cho ph√©p x·ª≠ l√Ω dataset c·ª±c l·ªõn (h√†ng tri·ªáu d√≤ng) m√† kh√¥ng
    c·∫ßn load to√†n b·ªô v√†o RAM.
    
    Example:
        >>> processor = BatchProcessor(max_memory_gb=4.0)
        >>> for batch_X, batch_y, batch_info in processor.iterate_batches(X, y):
        ...     # X·ª≠ l√Ω batch
        ...     process(batch_X, batch_y)
    """
    
    def __init__(
        self,
        max_memory_gb: float = 4.0,
        batch_size: Optional[int] = None,
        log_level: str = "INFO"
    ):
        """
        Kh·ªüi t·∫°o BatchProcessor.
        
        Args:
            max_memory_gb: RAM t·ªëi ƒëa ƒë∆∞·ª£c s·ª≠ d·ª•ng (GB)
            batch_size: S·ªë samples m·ªói batch (None = t·ª± ƒë·ªông)
            log_level: M·ª©c ƒë·ªô logging
        """
        self.memory_config = MemoryConfig(
            max_memory_gb=max_memory_gb,
            batch_size=batch_size
        )
        self.memory_manager = MemoryManager(self.memory_config)
        self.logger = get_logger("BatchProcessor", level=log_level, log_to_file=False)
    
    def iterate_batches(
        self,
        X: np.ndarray,
        y: Optional[np.ndarray] = None,
        batch_size: Optional[int] = None
    ) -> Generator[Tuple[np.ndarray, Optional[np.ndarray], dict], None, None]:
        """
        Iterate qua d·ªØ li·ªáu theo batch.
        
        Args:
            X: Features array
            y: Labels array (optional)
            batch_size: K√≠ch th∆∞·ªõc batch (None = t·ª± ƒë·ªông)
        
        Yields:
            Tuple[batch_X, batch_y, batch_info]
            - batch_X: Features c·ªßa batch
            - batch_y: Labels c·ªßa batch (None n·∫øu kh√¥ng c√≥ y)
            - batch_info: Dict ch·ª©a th√¥ng tin batch (start_idx, end_idx, batch_num)
        """
        n_samples = X.shape[0]
        n_features = X.shape[1]
        
        # T√≠nh batch size
        if batch_size is None:
            batch_size = self.memory_manager.calculate_optimal_batch_size(n_features)
        
        n_batches = (n_samples + batch_size - 1) // batch_size
        
        self.logger.info(f"üì¶ Batch processing: {n_samples:,} samples, {n_batches} batches, size={batch_size:,}")
        
        for batch_num in range(n_batches):
            start_idx = batch_num * batch_size
            end_idx = min(start_idx + batch_size, n_samples)
            
            batch_X = X[start_idx:end_idx]
            batch_y = y[start_idx:end_idx] if y is not None else None
            
            batch_info = {
                'batch_num': batch_num,
                'start_idx': start_idx,
                'end_idx': end_idx,
                'n_samples': end_idx - start_idx,
                'total_batches': n_batches
            }
            
            yield batch_X, batch_y, batch_info
            
            # Garbage collection sau m·ªói batch
            if batch_num % 10 == 0:
                self.memory_manager.force_garbage_collection()
    
    def process_in_batches(
        self,
        X: np.ndarray,
        process_func: Callable[[np.ndarray], np.ndarray],
        batch_size: Optional[int] = None
    ) -> np.ndarray:
        """
        X·ª≠ l√Ω d·ªØ li·ªáu theo batch v√† gh√©p k·∫øt qu·∫£.
        
        Args:
            X: Input array
            process_func: H√†m x·ª≠ l√Ω m·ªói batch
            batch_size: K√≠ch th∆∞·ªõc batch
        
        Returns:
            np.ndarray: K·∫øt qu·∫£ ƒë√£ gh√©p
        """
        results = []
        
        for batch_X, _, batch_info in self.iterate_batches(X, batch_size=batch_size):
            batch_result = process_func(batch_X)
            results.append(batch_result)
            
            self.logger.debug(
                f"   Batch {batch_info['batch_num']+1}/{batch_info['total_batches']}"
            )
        
        return np.concatenate(results, axis=0)
    
    def calculate_semantic_values_batched(
        self,
        X: np.ndarray,
        batch_size: Optional[int] = None
    ) -> np.ndarray:
        """
        T√≠nh semantic values theo batch.
        
        Args:
            X: Features array
            batch_size: K√≠ch th∆∞·ªõc batch
        
        Returns:
            np.ndarray: Semantic values
        """
        return self.process_in_batches(
            X,
            lambda batch: np.mean(batch, axis=1),
            batch_size=batch_size
        )


class ChunkedFileReader:
    """
    ƒê·ªçc file l·ªõn theo chunks ƒë·ªÉ tr√°nh tr√†n RAM.
    
    H·ªó tr·ª£ CSV v√† NPY files.
    """
    
    def __init__(
        self,
        file_path: str,
        chunk_size: int = 10000,
        max_memory_gb: float = 4.0
    ):
        self.file_path = file_path
        self.chunk_size = chunk_size
        self.max_memory_gb = max_memory_gb
        self.logger = get_logger("ChunkedReader", level="INFO", log_to_file=False)
    
    def read_csv_chunks(
        self,
        label_column: Optional[str] = None
    ) -> Generator[Tuple[np.ndarray, np.ndarray], None, None]:
        """
        ƒê·ªçc CSV theo chunks.
        
        Args:
            label_column: T√™n c·ªôt label
        
        Yields:
            Tuple[X_chunk, y_chunk]
        """
        import pandas as pd
        
        self.logger.info(f"üìÇ ƒê·ªçc file theo chunks: {self.file_path}")
        
        for chunk in pd.read_csv(self.file_path, chunksize=self.chunk_size):
            if label_column is not None:
                y = chunk[label_column].values
                X = chunk.drop(columns=[label_column]).values
            else:
                y = chunk.iloc[:, -1].values
                X = chunk.iloc[:, :-1].values
            
            yield X.astype(np.float64), y
    
    def count_rows(self) -> int:
        """ƒê·∫øm s·ªë d√≤ng trong file (kh√¥ng load v√†o RAM)."""
        import subprocess
        result = subprocess.run(['wc', '-l', self.file_path], capture_output=True, text=True)
        return int(result.stdout.split()[0]) - 1  # Tr·ª´ header


def estimate_memory_requirement(
    n_samples: int,
    n_features: int,
    n_clusters: int,
    dtype=np.float64
) -> float:
    """
    ∆Ø·ªõc t√≠nh RAM c·∫ßn thi·∫øt (GB).
    
    Args:
        n_samples: S·ªë samples
        n_features: S·ªë features
        n_clusters: S·ªë c·ª•m
        dtype: Ki·ªÉu d·ªØ li·ªáu
    
    Returns:
        float: RAM c·∫ßn thi·∫øt (GB)
    """
    bytes_per_element = np.dtype(dtype).itemsize
    
    # B·ªô nh·ªõ cho d·ªØ li·ªáu
    data_memory = n_samples * n_features * bytes_per_element
    
    # B·ªô nh·ªõ cho labels v√† semantic values
    labels_memory = n_samples * bytes_per_element * 2
    
    # B·ªô nh·ªõ cho c√°c bi·∫øn trung gian
    intermediate_memory = data_memory * 0.5
    
    total_bytes = data_memory + labels_memory + intermediate_memory
    
    return total_bytes / (1024**3)

