"""
Data Loader - Load vÃ  tiá»n xá»­ lÃ½ dá»¯ liá»‡u

Há»— trá»£:
- Load tá»« CSV, NPY
- Chuáº©n hÃ³a dá»¯ liá»‡u (Min-Max, Z-Score)
- Feature selection (loáº¡i bá» cá»™t constant)
- TÃ­nh Information Gain Ratio
- Split train/test
"""

import numpy as np
import pandas as pd
from typing import Tuple, Optional, Union, List
from pathlib import Path
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler, StandardScaler

from .logger import get_logger


class DataLoader:
    """
    Class Ä‘á»ƒ load vÃ  tiá»n xá»­ lÃ½ dá»¯ liá»‡u cho Hedge Algebra Clustering.
    
    Attributes:
        logger: Logger instance
        scaler: Scaler Ä‘Ã£ Ä‘Æ°á»£c fit (náº¿u cÃ³)
        removed_columns: Danh sÃ¡ch cÃ¡c cá»™t Ä‘Ã£ bá»‹ loáº¡i bá»
        information_gain_ratio: IG ratio cá»§a cÃ¡c features
    
    Example:
        >>> loader = DataLoader()
        >>> X_train, X_test, y_train, y_test = loader.load_and_split("data.csv")
        >>> print(f"Train: {X_train.shape}, Test: {X_test.shape}")
    """
    
    def __init__(self, log_level: str = "INFO"):
        """
        Khá»Ÿi táº¡o DataLoader.
        
        Args:
            log_level: Má»©c Ä‘á»™ logging
        """
        self.logger = get_logger("DataLoader", level=log_level, log_to_file=False)
        self.scaler = None
        self.removed_columns = []
        self.information_gain_ratio = None
        self._feature_names = None
    
    def load_csv(
        self,
        file_path: str,
        label_column: Optional[str] = None,
        label_column_index: int = -1
    ) -> Tuple[np.ndarray, np.ndarray]:
        """
        Load dá»¯ liá»‡u tá»« file CSV.
        
        Args:
            file_path: ÄÆ°á»ng dáº«n tá»›i file CSV
            label_column: TÃªn cá»™t label (náº¿u biáº¿t)
            label_column_index: Index cá»™t label (-1 = cá»™t cuá»‘i)
        
        Returns:
            Tuple[X, y]: Features vÃ  labels
        
        Raises:
            FileNotFoundError: Náº¿u file khÃ´ng tá»“n táº¡i
            ValueError: Náº¿u dá»¯ liá»‡u khÃ´ng há»£p lá»‡
        """
        file_path = Path(file_path)
        if not file_path.exists():
            raise FileNotFoundError(f"KhÃ´ng tÃ¬m tháº¥y file: {file_path}")
        
        self.logger.info(f"ğŸ“‚ Äang load file: {file_path.name}")
        
        # Load CSV
        dataframe = pd.read_csv(file_path)
        self._feature_names = list(dataframe.columns)
        
        # XÃ¡c Ä‘á»‹nh cá»™t label
        if label_column is not None:
            if label_column not in dataframe.columns:
                raise ValueError(f"KhÃ´ng tÃ¬m tháº¥y cá»™t '{label_column}' trong dá»¯ liá»‡u")
            y = dataframe[label_column].values
            X = dataframe.drop(columns=[label_column]).values
            self._feature_names.remove(label_column)
        else:
            # Máº·c Ä‘á»‹nh: cá»™t cuá»‘i lÃ  label
            y = dataframe.iloc[:, label_column_index].values
            X = dataframe.iloc[:, :label_column_index].values if label_column_index == -1 else \
                np.delete(dataframe.values, label_column_index, axis=1)
            self._feature_names = self._feature_names[:label_column_index] if label_column_index == -1 else \
                self._feature_names[:label_column_index] + self._feature_names[label_column_index+1:]
        
        self._log_data_info(X, y)
        return X.astype(np.float64), y
    
    def load_numpy(
        self,
        file_path: str,
        label_column_index: int = -1
    ) -> Tuple[np.ndarray, np.ndarray]:
        """
        Load dá»¯ liá»‡u tá»« file NPY.
        
        Args:
            file_path: ÄÆ°á»ng dáº«n tá»›i file NPY
            label_column_index: Index cá»™t label (-1 = cá»™t cuá»‘i)
        
        Returns:
            Tuple[X, y]: Features vÃ  labels
        """
        file_path = Path(file_path)
        if not file_path.exists():
            raise FileNotFoundError(f"KhÃ´ng tÃ¬m tháº¥y file: {file_path}")
        
        self.logger.info(f"ğŸ“‚ Äang load file: {file_path.name}")
        
        data = np.load(file_path, allow_pickle=True)
        
        if label_column_index == -1:
            X = data[:, :-1]
            y = data[:, -1]
        else:
            y = data[:, label_column_index]
            X = np.delete(data, label_column_index, axis=1)
        
        self._log_data_info(X, y)
        return X.astype(np.float64), y
    
    def _log_data_info(self, X: np.ndarray, y: np.ndarray):
        """Ghi log thÃ´ng tin dá»¯ liá»‡u."""
        n_samples, n_features = X.shape
        unique_classes = np.unique(y)
        n_classes = len(unique_classes)
        
        self.logger.info(f"  ğŸ“Š Sá»‘ samples: {n_samples:,}")
        self.logger.info(f"  ğŸ“Š Sá»‘ features: {n_features:,}")
        self.logger.info(f"  ğŸ“Š Sá»‘ classes: {n_classes} {list(unique_classes)[:5]}{'...' if n_classes > 5 else ''}")
    
    def normalize(
        self,
        X: np.ndarray,
        method: str = "minmax",
        fit: bool = True
    ) -> np.ndarray:
        """
        Chuáº©n hÃ³a dá»¯ liá»‡u.
        
        Args:
            X: Dá»¯ liá»‡u cáº§n chuáº©n hÃ³a
            method: PhÆ°Æ¡ng phÃ¡p chuáº©n hÃ³a ("minmax" hoáº·c "zscore")
            fit: True náº¿u cáº§n fit scaler, False náº¿u dÃ¹ng scaler Ä‘Ã£ fit
        
        Returns:
            np.ndarray: Dá»¯ liá»‡u Ä‘Ã£ chuáº©n hÃ³a
        """
        self.logger.info(f"ğŸ”§ Chuáº©n hÃ³a dá»¯ liá»‡u: {method}")
        
        if fit:
            if method == "minmax":
                self.scaler = MinMaxScaler()
            elif method == "zscore":
                self.scaler = StandardScaler()
            else:
                raise ValueError(f"PhÆ°Æ¡ng phÃ¡p khÃ´ng há»£p lá»‡: {method}")
            
            X_normalized = self.scaler.fit_transform(X)
        else:
            if self.scaler is None:
                raise ValueError("Scaler chÆ°a Ä‘Æ°á»£c fit. Gá»i normalize() vá»›i fit=True trÆ°á»›c.")
            X_normalized = self.scaler.transform(X)
        
        return X_normalized
    
    def remove_constant_columns(
        self,
        X: np.ndarray,
        fit: bool = True
    ) -> np.ndarray:
        """
        Loáº¡i bá» cÃ¡c cá»™t chá»‰ cÃ³ 1 giÃ¡ trá»‹ (constant columns).
        
        Args:
            X: Dá»¯ liá»‡u Ä‘áº§u vÃ o
            fit: True náº¿u cáº§n xÃ¡c Ä‘á»‹nh cÃ¡c cá»™t cáº§n loáº¡i, False náº¿u dÃ¹ng list Ä‘Ã£ xÃ¡c Ä‘á»‹nh
        
        Returns:
            np.ndarray: Dá»¯ liá»‡u sau khi loáº¡i bá» cá»™t constant
        """
        if fit:
            self.removed_columns = []
            for column_index in range(X.shape[1]):
                unique_values = np.unique(X[:, column_index])
                if len(unique_values) == 1:
                    self.removed_columns.append(column_index)
            
            if self.removed_columns:
                self.logger.info(f"ğŸ—‘ï¸  Loáº¡i bá» {len(self.removed_columns)} cá»™t constant: {self.removed_columns}")
        
        if self.removed_columns:
            X = np.delete(X, self.removed_columns, axis=1)
        
        return X
    
    def calculate_information_gain_ratio(
        self,
        X: np.ndarray,
        y: np.ndarray
    ) -> np.ndarray:
        """
        TÃ­nh Information Gain Ratio cho cÃ¡c features.
        
        Information Gain Ratio = Information Gain / Entropy cá»§a feature
        GiÃ¡ trá»‹ cao hÆ¡n = feature quan trá»ng hÆ¡n
        
        Args:
            X: Features
            y: Labels
        
        Returns:
            np.ndarray: IG ratio cho má»—i feature
        """
        self.logger.info("ğŸ“Š TÃ­nh Information Gain Ratio...")
        
        n_samples, n_features = X.shape
        
        # TÃ­nh entropy cá»§a dataset
        dataset_entropy = self._calculate_entropy(y)
        
        information_gain_list = []
        entropy_feature_list = []
        
        for feature_index in range(n_features):
            feature_values = X[:, feature_index]
            unique_values = np.unique(feature_values)
            
            # TÃ­nh conditional entropy H(Y|X)
            conditional_entropy = 0
            feature_entropy = 0
            
            for value in unique_values:
                mask = feature_values == value
                subset_labels = y[mask]
                weight = np.sum(mask) / n_samples
                
                # H(Y|X=value)
                conditional_entropy += weight * self._calculate_entropy(subset_labels)
                
                # H(X)
                if weight > 0:
                    feature_entropy -= weight * np.log2(weight)
            
            # Information Gain = H(Y) - H(Y|X)
            information_gain = dataset_entropy - conditional_entropy
            information_gain_list.append(information_gain)
            entropy_feature_list.append(feature_entropy)
        
        # Information Gain Ratio = IG / H(X)
        entropy_feature_array = np.array(entropy_feature_list)
        entropy_feature_array[entropy_feature_array == 0] = 1e-10  # TrÃ¡nh chia cho 0
        
        self.information_gain_ratio = np.array(information_gain_list) / entropy_feature_array
        
        self.logger.info(f"  ğŸ“ˆ IG Ratio range: [{self.information_gain_ratio.min():.4f}, {self.information_gain_ratio.max():.4f}]")
        
        return self.information_gain_ratio
    
    def _calculate_entropy(self, y: np.ndarray) -> float:
        """TÃ­nh entropy cá»§a má»™t táº­p labels."""
        if len(y) == 0:
            return 0
        
        unique_classes, class_counts = np.unique(y, return_counts=True)
        probabilities = class_counts / len(y)
        
        # H(Y) = -Î£ p(y) * log2(p(y))
        entropy = -np.sum(probabilities * np.log2(probabilities + 1e-10))
        return entropy
    
    def apply_information_gain_weights(
        self,
        X: np.ndarray
    ) -> np.ndarray:
        """
        Ãp dá»¥ng IG ratio lÃ m trá»ng sá»‘ cho features.
        
        Args:
            X: Features
        
        Returns:
            np.ndarray: Features Ä‘Ã£ Ä‘Æ°á»£c weight
        """
        if self.information_gain_ratio is None:
            raise ValueError("ChÆ°a tÃ­nh IG ratio. Gá»i calculate_information_gain_ratio() trÆ°á»›c.")
        
        if X.shape[1] != len(self.information_gain_ratio):
            raise ValueError(
                f"Sá»‘ features ({X.shape[1]}) khÃ´ng khá»›p vá»›i sá»‘ IG ratio ({len(self.information_gain_ratio)})"
            )
        
        return X * self.information_gain_ratio
    
    def split_train_test(
        self,
        X: np.ndarray,
        y: np.ndarray,
        test_size: float = 0.2,
        random_state: int = 42,
        stratify: bool = True
    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
        """
        Chia dá»¯ liá»‡u thÃ nh train vÃ  test sets.
        
        Args:
            X: Features
            y: Labels
            test_size: Tá»· lá»‡ test set
            random_state: Seed
            stratify: CÃ³ giá»¯ tá»· lá»‡ classes khÃ´ng
        
        Returns:
            Tuple[X_train, X_test, y_train, y_test]
        """
        self.logger.info(f"âœ‚ï¸  Chia dá»¯ liá»‡u: train={1-test_size:.0%}, test={test_size:.0%}")
        
        stratify_param = y if stratify else None
        
        X_train, X_test, y_train, y_test = train_test_split(
            X, y,
            test_size=test_size,
            random_state=random_state,
            stratify=stratify_param
        )
        
        self.logger.info(f"  ğŸ“Š Train: {X_train.shape[0]:,} samples")
        self.logger.info(f"  ğŸ“Š Test: {X_test.shape[0]:,} samples")
        
        return X_train, X_test, y_train, y_test
    
    def load_and_preprocess(
        self,
        file_path: str,
        label_column: Optional[str] = None,
        normalize_method: str = "minmax",
        remove_constant: bool = True,
        calculate_ig: bool = False,
        test_size: float = 0.2,
        random_state: int = 42
    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, Optional[np.ndarray]]:
        """
        Pipeline Ä‘áº§y Ä‘á»§: Load â†’ Preprocess â†’ Split.
        
        ÄÃ¢y lÃ  method chÃ­nh Ä‘á»ƒ sá»­ dá»¥ng DataLoader.
        
        Args:
            file_path: ÄÆ°á»ng dáº«n tá»›i file dá»¯ liá»‡u (CSV hoáº·c NPY)
            label_column: TÃªn cá»™t label (cho CSV)
            normalize_method: PhÆ°Æ¡ng phÃ¡p chuáº©n hÃ³a
            remove_constant: CÃ³ loáº¡i bá» cá»™t constant khÃ´ng
            calculate_ig: CÃ³ tÃ­nh IG ratio khÃ´ng
            test_size: Tá»· lá»‡ test set
            random_state: Seed
        
        Returns:
            Tuple[X_train, X_test, y_train, y_test, ig_ratio]
        
        Example:
            >>> loader = DataLoader()
            >>> X_train, X_test, y_train, y_test, ig = loader.load_and_preprocess(
            ...     "data.csv",
            ...     normalize_method="minmax",
            ...     calculate_ig=True
            ... )
        """
        self.logger.info("=" * 60)
        self.logger.info("ğŸš€ Báº®T Äáº¦U LOAD VÃ€ TIá»€N Xá»¬ LÃ Dá»® LIá»†U")
        self.logger.info("=" * 60)
        
        # 1. Load dá»¯ liá»‡u
        file_path = Path(file_path)
        if file_path.suffix.lower() == '.csv':
            X, y = self.load_csv(str(file_path), label_column=label_column)
        elif file_path.suffix.lower() == '.npy':
            X, y = self.load_numpy(str(file_path))
        else:
            raise ValueError(f"KhÃ´ng há»— trá»£ Ä‘á»‹nh dáº¡ng: {file_path.suffix}")
        
        # 2. Loáº¡i bá» cá»™t constant
        if remove_constant:
            X = self.remove_constant_columns(X)
        
        # 3. Chuáº©n hÃ³a
        X = self.normalize(X, method=normalize_method)
        
        # 4. TÃ­nh IG ratio (náº¿u cáº§n)
        ig_ratio = None
        if calculate_ig:
            ig_ratio = self.calculate_information_gain_ratio(X, y)
        
        # 5. Split
        X_train, X_test, y_train, y_test = self.split_train_test(
            X, y, test_size=test_size, random_state=random_state
        )
        
        self.logger.info("=" * 60)
        self.logger.info("âœ… HOÃ€N Táº¤T TIá»€N Xá»¬ LÃ Dá»® LIá»†U")
        self.logger.info("=" * 60)
        
        return X_train, X_test, y_train, y_test, ig_ratio
    
    @property
    def feature_names(self) -> Optional[List[str]]:
        """Tráº£ vá» tÃªn cÃ¡c features (náº¿u cÃ³)."""
        return self._feature_names

