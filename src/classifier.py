"""
Classifier - Training v√† Testing v·ªõi ML models cho t·ª´ng c·ª•m

Module n√†y ch·ª©a:
- ClusterClassifier: Class ƒë·ªÉ train model cho t·ª´ng c·ª•m
- C√°c metrics ƒë√°nh gi√°
- H·ªó tr·ª£ l∆∞u/load models
"""

import numpy as np
import time
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
from pathlib import Path
import joblib

from sklearn.base import BaseEstimator
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    classification_report
)

from .clustering import HedgeAlgebraClustering, ClusteringResult
from .logger import get_logger, TrainingLogger


@dataclass
class PredictionResult:
    """
    K·∫øt qu·∫£ d·ª± ƒëo√°n.
    
    Attributes:
        predictions: Nh√£n d·ª± ƒëo√°n
        accuracy: ƒê·ªô ch√≠nh x√°c
        precision: Precision (macro average)
        recall: Recall (macro average)
        f1: F1-score (macro average)
        total_time: T·ªïng th·ªùi gian d·ª± ƒëo√°n
        average_time_per_sample: Th·ªùi gian trung b√¨nh m·ªói sample
        classification_report: B√°o c√°o chi ti·∫øt
    """
    predictions: np.ndarray
    accuracy: float
    precision: float
    recall: float
    f1: float
    total_time: float
    average_time_per_sample: float
    classification_report: str


class ClusterClassifier:
    """
    Classifier cho Hedge Algebra Clustering.
    
    Train m·ªôt ML model ri√™ng cho m·ªói c·ª•m, sau ƒë√≥ s·ª≠ d·ª•ng
    ph√¢n c·ª•m ƒë·ªÉ quy·∫øt ƒë·ªãnh model n√†o s·∫Ω ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ d·ª± ƒëo√°n.
    
    Attributes:
        clustering: HedgeAlgebraClustering instance
        base_classifier: ML model base (sklearn compatible)
        cluster_models: Dict ch·ª©a model c·ªßa t·ª´ng c·ª•m
    
    Example:
        >>> classifier = ClusterClassifier(
        ...     n_clusters=3,
        ...     base_classifier=GradientBoostingClassifier()
        ... )
        >>> classifier.fit(X_train, y_train)
        >>> result = classifier.predict(X_test, y_test)
        >>> print(f"Accuracy: {result.accuracy:.4f}")
    """

    def __init__(
        self,
        n_clusters: int = 2,
        theta: float = 0.5,
        alpha: float = 0.5,
        base_classifier: Optional[BaseEstimator] = None,
        use_information_gain: bool = False,
        random_state: int = 42,
        log_level: str = "INFO",
        center_init: str = "ver6"
    ):
        """
        Kh·ªüi t·∫°o ClusterClassifier.

        Args:
            n_clusters: S·ªë c·ª•m
            theta: Tham s·ªë theta cho ƒêSGT
            alpha: Tham s·ªë alpha cho ƒêSGT
            base_classifier: Model sklearn (m·∫∑c ƒë·ªãnh GradientBoostingClassifier)
            use_information_gain: C√≥ s·ª≠ d·ª•ng IG weights kh√¥ng
            random_state: Seed
            log_level: M·ª©c ƒë·ªô logging
            center_init: Ch·∫ø ƒë·ªô kh·ªüi t·∫°o t√¢m c·ª•m ("ver6" ho·∫∑c "legacy")
        """
        if center_init not in ("ver6", "legacy"):
            raise ValueError(
                f"center_init ph·∫£i l√† 'ver6' ho·∫∑c 'legacy', nh·∫≠n ƒë∆∞·ª£c: {center_init}"
            )

        self.n_clusters = n_clusters
        self.theta = theta
        self.alpha = alpha
        self.use_information_gain = use_information_gain
        self.random_state = random_state
        self.center_init = center_init

        # Logger
        self.logger = get_logger("Classifier", level=log_level, log_to_file=False)
        self.training_logger = TrainingLogger(self.logger)

        # ML model
        if base_classifier is None:
            self.base_classifier = GradientBoostingClassifier(random_state=random_state)
        else:
            self.base_classifier = base_classifier

        # Clustering
        self.clustering = HedgeAlgebraClustering(
            n_clusters=n_clusters,
            theta=theta,
            alpha=alpha,
            log_level=log_level,
            center_init=center_init
        )

        # Models cho t·ª´ng c·ª•m
        self.cluster_models: Dict[int, BaseEstimator] = {}
        self.information_gain_weights: Optional[np.ndarray] = None

        # K·∫øt qu·∫£ training
        self._clustering_result: Optional[ClusteringResult] = None
        self._is_fitted: bool = False
        self._training_time: float = 0.0

    def fit(
        self,
        X: np.ndarray,
        y: np.ndarray,
        information_gain_weights: Optional[np.ndarray] = None
    ) -> 'ClusterClassifier':
        """
        Train classifier tr√™n d·ªØ li·ªáu.
        
        Quy tr√¨nh:
        1. Ph√¢n c·ª•m d·ªØ li·ªáu training
        2. Train m·ªôt model ri√™ng cho m·ªói c·ª•m
        
        Args:
            X: Features (n_samples, n_features)
            y: Labels (n_samples,)
            information_gain_weights: Tr·ªçng s·ªë IG cho m·ªói feature
        
        Returns:
            self: ƒê·ªÉ c√≥ th·ªÉ chain methods
        
        Example:
            >>> classifier.fit(X_train, y_train).predict(X_test, y_test)
        """
        start_time = time.time()
        
        self.logger.info("=" * 60)
        self.logger.info("üöÄ B·∫ÆT ƒê·∫¶U TRAINING")
        self.logger.info("=" * 60)
        
        # L∆∞u IG weights
        if information_gain_weights is not None:
            self.information_gain_weights = information_gain_weights
            self.use_information_gain = True
        
        # Log th√¥ng tin d·ªØ li·ªáu
        n_samples, n_features = X.shape
        unique_classes = np.unique(y)
        n_classes = len(unique_classes)
        self.training_logger.log_data_info(n_samples, n_features, n_classes)
        
        # 1. Ph√¢n c·ª•m
        self.logger.info("\nüìç B∆∞·ªõc 1: Ph√¢n c·ª•m d·ªØ li·ªáu")
        
        # DEBUG: Ki·ªÉm tra X tr∆∞·ªõc khi clustering
        print("üîç DEBUG [Classifier] - NaN in X:", np.isnan(X).sum(), "Inf in X:", np.isinf(X).sum())
        print("üîç DEBUG [Classifier] - X shape:", X.shape, "X min/max:", np.nanmin(X), np.nanmax(X))
        
        self._clustering_result = self.clustering.fit(
            X, 
            information_gain_weights=self.information_gain_weights
        )
        
        cluster_labels = self._clustering_result.cluster_labels
        
        # Log ph√¢n b·ªë c·ª•m
        cluster_distribution = {}
        for cluster_id in range(1, self.n_clusters + 1):
            count = int(np.sum(cluster_labels == cluster_id))
            cluster_distribution[cluster_id] = count
        self.training_logger.log_cluster_distribution(cluster_distribution)
        
        # 2. Train model cho t·ª´ng c·ª•m
        self.logger.info("\nüìç B∆∞·ªõc 2: Training model cho t·ª´ng c·ª•m")
        
        # √Åp d·ª•ng IG weights n·∫øu c√≥
        X_weighted = X
        if self.use_information_gain and self.information_gain_weights is not None:
            X_weighted = X * self.information_gain_weights
        
        for cluster_id in range(1, self.n_clusters + 1):
            cluster_mask = (cluster_labels == cluster_id)
            X_cluster = X_weighted[cluster_mask]
            y_cluster = y[cluster_mask]
            
            n_cluster_samples = len(y_cluster)
            
            if n_cluster_samples == 0:
                self.logger.warning(f"   ‚ö†Ô∏è C·ª•m {cluster_id}: Kh√¥ng c√≥ d·ªØ li·ªáu!")
                continue
            
            unique_in_cluster = np.unique(y_cluster)
            
            if len(unique_in_cluster) < 2:
                self.logger.warning(
                    f"   ‚ö†Ô∏è C·ª•m {cluster_id}: Ch·ªâ c√≥ 1 class ({unique_in_cluster[0]}), "
                    f"s·ª≠ d·ª•ng constant predictor"
                )
                # T·∫°o m·ªôt "model" ƒë∆°n gi·∫£n tr·∫£ v·ªÅ class duy nh·∫•t
                from sklearn.dummy import DummyClassifier
                # √âp ki·ªÉu int n·∫øu c√≥ th·ªÉ, n·∫øu kh√¥ng gi·ªØ nguy√™n
                constant_value = unique_in_cluster[0]
                if isinstance(constant_value, (np.floating, float)):
                    constant_value = int(constant_value)
                model = DummyClassifier(strategy='constant', constant=constant_value)
                model.fit(X_cluster, y_cluster)
                self.cluster_models[cluster_id] = model
                self.training_logger.log_training_result(cluster_id, success=True)
                continue
            
            try:
                # Clone model ƒë·ªÉ m·ªói c·ª•m c√≥ model ri√™ng
                from sklearn.base import clone
                model = clone(self.base_classifier)
                model.fit(X_cluster, y_cluster)
                self.cluster_models[cluster_id] = model
                
                self.training_logger.log_training_result(cluster_id, success=True)
                self.logger.debug(
                    f"      Samples: {n_cluster_samples}, Classes: {len(unique_in_cluster)}"
                )
                
            except Exception as e:
                # Fallback: d√πng DummyClassifier
                self.logger.warning(f"   ‚ö†Ô∏è C·ª•m {cluster_id}: Fallback to DummyClassifier")
                from sklearn.dummy import DummyClassifier
                model = DummyClassifier(strategy='most_frequent')
                model.fit(X_cluster, y_cluster)
                self.cluster_models[cluster_id] = model
                self.training_logger.log_training_result(cluster_id, success=True)
        
        self._is_fitted = True
        self._training_time = time.time() - start_time
        
        self.logger.info(f"\n‚è±Ô∏è Th·ªùi gian training: {self._training_time:.2f}s")
        self.logger.info("=" * 60)
        self.logger.info("‚úÖ HO√ÄN T·∫§T TRAINING")
        self.logger.info("=" * 60)
        
        return self
    
    def predict(
        self,
        X: np.ndarray,
        y_true: Optional[np.ndarray] = None
    ) -> PredictionResult:
        """
        D·ª± ƒëo√°n tr√™n d·ªØ li·ªáu m·ªõi.
        
        Quy tr√¨nh:
        1. X√°c ƒë·ªãnh c·ª•m c·ªßa m·ªói sample
        2. S·ª≠ d·ª•ng model c·ªßa c·ª•m ƒë√≥ ƒë·ªÉ d·ª± ƒëo√°n
        
        Args:
            X: Features
            y_true: Labels th·ª±c (n·∫øu c√≥, ƒë·ªÉ t√≠nh metrics)
        
        Returns:
            PredictionResult: K·∫øt qu·∫£ d·ª± ƒëo√°n v√† metrics
        
        Raises:
            ValueError: N·∫øu ch∆∞a fit
        """
        if not self._is_fitted:
            raise ValueError("Classifier ch∆∞a ƒë∆∞·ª£c fit. G·ªçi fit() tr∆∞·ªõc.")
        
        self.logger.info("=" * 60)
        self.logger.info("üîÆ B·∫ÆT ƒê·∫¶U D·ª∞ ƒêO√ÅN")
        self.logger.info("=" * 60)
        
        n_samples = X.shape[0]
        self.logger.info(f"   üìä S·ªë samples: {n_samples:,}")
        
        # √Åp d·ª•ng IG weights
        X_weighted = X
        if self.use_information_gain and self.information_gain_weights is not None:
            X_weighted = X * self.information_gain_weights
        
        # FIX: Label c√≥ th·ªÉ l√† string (vd: 'DDoS-ICMP_Flood'), n√™n kh√¥ng ƒë∆∞·ª£c d√πng float array
        # D√πng dtype theo y_true n·∫øu c√≥, fallback object
        pred_dtype = y_true.dtype if y_true is not None else object
        predictions = np.empty(n_samples, dtype=pred_dtype)
        prediction_times = []
        
        # D·ª± ƒëo√°n t·ª´ng sample
        for sample_index in range(n_samples):
            start_time = time.time()
            
            sample = X_weighted[sample_index:sample_index+1]
            
            # X√°c ƒë·ªãnh c·ª•m
            cluster_label = self.clustering.predict(sample)[0]
            cluster_id = int(cluster_label)
            
            # D·ª± ƒëo√°n b·∫±ng model c·ªßa c·ª•m
            if cluster_id in self.cluster_models:
                prediction = self.cluster_models[cluster_id].predict(sample)[0]
            else:
                # Fallback: d√πng model c·ªßa c·ª•m ƒë·∫ßu ti√™n c√≥ s·∫µn
                first_available_cluster = min(self.cluster_models.keys())
                prediction = self.cluster_models[first_available_cluster].predict(sample)[0]
                self.logger.debug(
                    f"   ‚ö†Ô∏è Sample {sample_index}: C·ª•m {cluster_id} kh√¥ng c√≥ model, "
                    f"d√πng c·ª•m {first_available_cluster}"
                )
            
            predictions[sample_index] = prediction
            prediction_times.append(time.time() - start_time)
        
        total_time = sum(prediction_times)
        average_time = np.mean(prediction_times)
        
        # T√≠nh metrics n·∫øu c√≥ y_true
        if y_true is not None:
            accuracy = accuracy_score(y_true, predictions)
            precision = precision_score(y_true, predictions, average='macro', zero_division=0)
            recall = recall_score(y_true, predictions, average='macro', zero_division=0)
            f1 = f1_score(y_true, predictions, average='macro', zero_division=0)
            report = classification_report(y_true, predictions, digits=4, zero_division=0)
            
            self.training_logger.log_metrics({
                'Accuracy': accuracy,
                'Precision (macro)': precision,
                'Recall (macro)': recall,
                'F1-score (macro)': f1
            })
            
            self.training_logger.log_summary(
                train_time=self._training_time,
                test_time=total_time,
                accuracy=accuracy
            )
        else:
            accuracy = precision = recall = f1 = 0.0
            report = "Kh√¥ng c√≥ y_true ƒë·ªÉ t√≠nh metrics"
        
        self.logger.info("=" * 60)
        self.logger.info("‚úÖ HO√ÄN T·∫§T D·ª∞ ƒêO√ÅN")
        self.logger.info("=" * 60)
        
        return PredictionResult(
            predictions=predictions,
            accuracy=accuracy,
            precision=precision,
            recall=recall,
            f1=f1,
            total_time=total_time,
            average_time_per_sample=average_time,
            classification_report=report
        )
    
    def save_models(self, directory: str = "models"):
        """
        L∆∞u c√°c models ƒë√£ train.
        
        Args:
            directory: Th∆∞ m·ª•c l∆∞u models
        """
        directory = Path(directory)
        directory.mkdir(parents=True, exist_ok=True)
        
        for cluster_id, model in self.cluster_models.items():
            model_path = directory / f"cluster_{cluster_id}_model.pkl"
            joblib.dump(model, model_path)
            self.logger.info(f"   üíæ ƒê√£ l∆∞u model c·ª•m {cluster_id}: {model_path}")
        
        # L∆∞u metadata
        metadata = {
            'n_clusters': self.n_clusters,
            'theta': self.theta,
            'alpha': self.alpha,
            'cluster_centers': self.clustering.cluster_centers,
            'information_gain_weights': self.information_gain_weights
        }
        metadata_path = directory / "metadata.pkl"
        joblib.dump(metadata, metadata_path)
        self.logger.info(f"   üíæ ƒê√£ l∆∞u metadata: {metadata_path}")
    
    def load_models(self, directory: str = "models"):
        """
        Load c√°c models ƒë√£ l∆∞u.
        
        Args:
            directory: Th∆∞ m·ª•c ch·ª©a models
        """
        directory = Path(directory)
        
        # Load metadata
        metadata_path = directory / "metadata.pkl"
        metadata = joblib.load(metadata_path)
        
        self.n_clusters = metadata['n_clusters']
        self.theta = metadata['theta']
        self.alpha = metadata['alpha']
        self.information_gain_weights = metadata['information_gain_weights']
        self.clustering._cluster_centers = metadata['cluster_centers']
        self.clustering._is_fitted = True
        
        # Load models
        for cluster_id in range(1, self.n_clusters + 1):
            model_path = directory / f"cluster_{cluster_id}_model.pkl"
            if model_path.exists():
                self.cluster_models[cluster_id] = joblib.load(model_path)
                self.logger.info(f"   üìÇ ƒê√£ load model c·ª•m {cluster_id}")
        
        self._is_fitted = True
        self.logger.info("   ‚úÖ Load models th√†nh c√¥ng")
    
    @property
    def training_time(self) -> float:
        """Tr·∫£ v·ªÅ th·ªùi gian training."""
        return self._training_time
    
    @property
    def cluster_centers(self) -> Optional[List[float]]:
        """Tr·∫£ v·ªÅ t√¢m c·ª•m."""
        return self.clustering.cluster_centers

