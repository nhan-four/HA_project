"""
Clustering Evaluation Metrics - C√°c ch·ªâ s·ªë ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng ph√¢n c·ª•m

Bao g·ªìm:
- Partition Coefficient (PC): ƒêo l∆∞·ªùng m·ª©c ƒë·ªô ph√¢n bi·ªát v√† ƒë·ªô t·∫≠p trung c·ªßa c√°c c·ª•m m·ªù
- Classification Entropy (CE): ƒêo l∆∞·ªùng m·ª©c ƒë·ªô kh√¥ng ch·∫Øc ch·∫Øn trong vi·ªác ph√¢n c·ª•m
- Xie-Beni Index (XB): ƒêo l∆∞·ªùng ch·∫•t l∆∞·ª£ng ph√¢n c·ª•m d·ª±a tr√™n t·ª∑ l·ªá compactness/separation
"""

import numpy as np
from typing import List, Tuple, Optional
from dataclasses import dataclass

from .logger import get_logger


@dataclass
class ClusteringMetrics:
    """
    K·∫øt qu·∫£ c√°c ch·ªâ s·ªë ƒë√°nh gi√° ph√¢n c·ª•m.
    
    Attributes:
        partition_coefficient: Partition Coefficient (PC) - c√†ng cao c√†ng t·ªët (0-1)
        classification_entropy: Classification Entropy (CE) - c√†ng th·∫•p c√†ng t·ªët (0-1)
        xie_beni_index: Xie-Beni Index (XB) - c√†ng th·∫•p c√†ng t·ªët (>0)
        silhouette_score: Silhouette Score (n·∫øu c√≥)
    """
    partition_coefficient: float
    classification_entropy: float
    xie_beni_index: float
    silhouette_score: float = 0.0
    
    def summary(self) -> str:
        """T·∫°o b·∫£ng t√≥m t·∫Øt metrics."""
        lines = [
            "=" * 60,
            "üìä CLUSTERING EVALUATION METRICS",
            "=" * 60,
            f"",
            f"Partition Coefficient (PC):     {self.partition_coefficient:.6f} (‚Üë cao h∆°n = t·ªët h∆°n)",
            f"Classification Entropy (CE):     {self.classification_entropy:.6f} (‚Üì th·∫•p h∆°n = t·ªët h∆°n)",
            f"Xie-Beni Index (XB):            {self.xie_beni_index:.6f} (‚Üì th·∫•p h∆°n = t·ªët h∆°n)",
            f"Silhouette Score:                {self.silhouette_score:.6f} (‚Üë cao h∆°n = t·ªët h∆°n)",
            f"",
            "=" * 60
        ]
        return "\n".join(lines)


class ClusteringEvaluator:
    """
    Class ƒë·ªÉ t√≠nh c√°c ch·ªâ s·ªë ƒë√°nh gi√° ph√¢n c·ª•m.
    
    H·ªó tr·ª£ ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng ph√¢n c·ª•m d·ª±a tr√™n:
    - Membership matrix (ma tr·∫≠n ƒë·ªô thu·ªôc)
    - Cluster centers (t√¢m c·ª•m)
    - Data points (ƒëi·ªÉm d·ªØ li·ªáu)
    
    Example:
        >>> evaluator = ClusteringEvaluator()
        >>> metrics = evaluator.evaluate(X, cluster_labels, cluster_centers)
        >>> print(metrics.summary())
    """
    
    def __init__(self, log_level: str = "WARNING"):
        """
        Kh·ªüi t·∫°o ClusteringEvaluator.
        
        Args:
            log_level: M·ª©c ƒë·ªô logging
        """
        self.logger = get_logger("ClusteringEvaluator", level=log_level, log_to_file=False)
    
    def calculate_membership_matrix(
        self,
        semantic_values: np.ndarray,
        cluster_centers: np.ndarray,
        fuzziness: float = 2.0
    ) -> np.ndarray:
        """
        T√≠nh ma tr·∫≠n membership (ƒë·ªô thu·ªôc) cho fuzzy clustering.
        
        Membership ƒë∆∞·ª£c t√≠nh d·ª±a tr√™n kho·∫£ng c√°ch t·ª´ ƒëi·ªÉm ƒë·∫øn t√¢m c·ª•m.
        S·ª≠ d·ª•ng c√¥ng th·ª©c t∆∞∆°ng t·ª± Fuzzy C-Means.
        
        Args:
            semantic_values: Gi√° tr·ªã ng·ªØ nghƒ©a c·ªßa c√°c ƒëi·ªÉm (n_samples,)
            cluster_centers: T√¢m c√°c c·ª•m (n_clusters,) - c√≥ th·ªÉ l√† list ho·∫∑c array
            fuzziness: Tham s·ªë m·ªù (m > 1), m·∫∑c ƒë·ªãnh 2.0
        
        Returns:
            np.ndarray: Ma tr·∫≠n membership (n_clusters, n_samples)
                       M·ªói c·ªôt t·ªïng = 1 (probabilistic)
        """
        # Convert cluster_centers sang numpy array n·∫øu l√† list
        cluster_centers = np.array(cluster_centers)
        semantic_values = np.array(semantic_values)
        
        n_samples = len(semantic_values)
        n_clusters = len(cluster_centers)
        
        # T√≠nh kho·∫£ng c√°ch t·ª´ m·ªói ƒëi·ªÉm ƒë·∫øn m·ªói t√¢m c·ª•m
        distances = np.abs(
            semantic_values[:, np.newaxis] - cluster_centers[np.newaxis, :]
        )
        
        # Tr√°nh chia cho 0
        distances = np.maximum(distances, 1e-10)
        
        # T√≠nh membership theo c√¥ng th·ª©c Fuzzy C-Means
        # u_ij = 1 / sum_k (d_ij / d_kj)^(2/(m-1))
        power = 2.0 / (fuzziness - 1.0)
        membership = np.zeros((n_clusters, n_samples))
        
        for i in range(n_clusters):
            for j in range(n_samples):
                ratio = distances[j, i] / distances[j, :]
                membership[i, j] = 1.0 / np.sum(ratio ** power)
        
        # Chu·∫©n h√≥a ƒë·ªÉ ƒë·∫£m b·∫£o t·ªïng = 1
        membership = membership / np.sum(membership, axis=0, keepdims=True)
        
        return membership
    
    def calculate_partition_coefficient(
        self,
        membership_matrix: np.ndarray
    ) -> float:
        """
        T√≠nh Partition Coefficient (PC).
        
        PC ƒëo l∆∞·ªùng m·ª©c ƒë·ªô ph√¢n bi·ªát v√† ƒë·ªô t·∫≠p trung c·ªßa c√°c c·ª•m m·ªù.
        Gi√° tr·ªã PC n·∫±m trong kho·∫£ng [1/n_clusters, 1]:
        - PC = 1/n_clusters: C√°c c·ª•m ho√†n to√†n kh√¥ng ph√¢n bi·ªát (worst)
        - PC = 1: C√°c c·ª•m ho√†n to√†n ph√¢n bi·ªát (best)
        
        C√¥ng th·ª©c: PC = (1/n) * sum_i sum_j (u_ij)^2
        
        Args:
            membership_matrix: Ma tr·∫≠n membership (n_clusters, n_samples)
        
        Returns:
            float: Partition Coefficient (0-1)
        """
        n_samples = membership_matrix.shape[1]
        
        # PC = (1/n) * sum_i sum_j (u_ij)^2
        pc = np.mean(np.sum(membership_matrix ** 2, axis=0))
        
        return float(pc)
    
    def calculate_classification_entropy(
        self,
        membership_matrix: np.ndarray
    ) -> float:
        """
        T√≠nh Classification Entropy (CE).
        
        CE ƒëo l∆∞·ªùng m·ª©c ƒë·ªô kh√¥ng ch·∫Øc ch·∫Øn trong vi·ªác ph√¢n c·ª•m.
        Gi√° tr·ªã CE n·∫±m trong kho·∫£ng [0, log(n_clusters)]:
        - CE = 0: Ph√¢n c·ª•m ho√†n to√†n ch·∫Øc ch·∫Øn (best)
        - CE = log(n_clusters): Ph√¢n c·ª•m ho√†n to√†n kh√¥ng ch·∫Øc ch·∫Øn (worst)
        
        C√¥ng th·ª©c: CE = -(1/n) * sum_i sum_j (u_ij * log(u_ij))
        
        Args:
            membership_matrix: Ma tr·∫≠n membership (n_clusters, n_samples)
        
        Returns:
            float: Classification Entropy (0-1, normalized)
        """
        n_samples = membership_matrix.shape[1]
        n_clusters = membership_matrix.shape[0]
        
        # Tr√°nh log(0)
        membership_safe = np.maximum(membership_matrix, 1e-10)
        
        # CE = -(1/n) * sum_i sum_j (u_ij * log(u_ij))
        ce = -np.mean(np.sum(membership_safe * np.log(membership_safe), axis=0))
        
        # Normalize v·ªÅ [0, 1]
        max_ce = np.log(n_clusters)
        if max_ce > 0:
            ce_normalized = ce / max_ce
        else:
            ce_normalized = 0.0
        
        return float(ce_normalized)
    
    def calculate_xie_beni_index(
        self,
        X: np.ndarray,
        membership_matrix: np.ndarray,
        cluster_centers: np.ndarray
    ) -> float:
        """
        T√≠nh Xie-Beni Index (XB).
        
        XB ƒëo l∆∞·ªùng ch·∫•t l∆∞·ª£ng ph√¢n c·ª•m d·ª±a tr√™n t·ª∑ l·ªá:
        - Compactness: ƒê·ªô ch·∫∑t trong t·ª´ng c·ª•m (numerator)
        - Separation: Kho·∫£ng c√°ch gi·ªØa c√°c c·ª•m (denominator)
        
        XB = (sum_i sum_j (u_ij^2 * ||x_j - v_i||^2)) / (n * min_{i!=k} ||v_i - v_k||^2)
        
        Gi√° tr·ªã XB c√†ng th·∫•p c√†ng t·ªët:
        - XB th·∫•p: C√°c c·ª•m ch·∫∑t ch·∫Ω v√† t√°ch bi·ªát r√µ r√†ng
        
        Args:
            X: Features array (n_samples, n_features)
            membership_matrix: Ma tr·∫≠n membership (n_clusters, n_samples)
            cluster_centers: T√¢m c·ª•m (n_clusters,) - semantic values c·ªßa centers
        
        Returns:
            float: Xie-Beni Index (>0)
        """
        n_samples = X.shape[0]
        n_clusters = len(cluster_centers)
        
        # T√≠nh semantic values c·ªßa X
        semantic_values = np.mean(X, axis=1)
        
        # T·ª≠ s·ªë: T·ªïng compactness
        # sum_i sum_j (u_ij^2 * ||x_j - v_i||^2)
        numerator = 0.0
        
        for i in range(n_clusters):
            for j in range(n_samples):
                distance = abs(semantic_values[j] - cluster_centers[i])
                numerator += (membership_matrix[i, j] ** 2) * (distance ** 2)
        
        # M·∫´u s·ªë: Separation (kho·∫£ng c√°ch nh·ªè nh·∫•t gi·ªØa c√°c t√¢m c·ª•m)
        min_center_distance = np.inf
        
        for i in range(n_clusters):
            for k in range(n_clusters):
                if i != k:
                    distance = abs(cluster_centers[i] - cluster_centers[k])
                    if distance < min_center_distance:
                        min_center_distance = distance
        
        # Tr√°nh chia cho 0
        if min_center_distance < 1e-10:
            min_center_distance = 1e-10
        
        denominator = n_samples * (min_center_distance ** 2)
        
        # XB Index
        xb = numerator / denominator
        
        return float(xb)
    
    def calculate_silhouette_score(
        self,
        semantic_values: np.ndarray,
        cluster_labels: np.ndarray,
        cluster_centers: np.ndarray,
        sample_size: int = 5000
    ) -> float:
        """
        T√≠nh Silhouette Score (approximate).
        
        S·ª≠ d·ª•ng sampling n·∫øu dataset l·ªõn ƒë·ªÉ tƒÉng t·ªëc.
        
        Args:
            semantic_values: Gi√° tr·ªã ng·ªØ nghƒ©a
            cluster_labels: Nh√£n c·ª•m
            cluster_centers: T√¢m c·ª•m
            sample_size: S·ªë samples ƒë·ªÉ t√≠nh (n·∫øu dataset l·ªõn)
        
        Returns:
            float: Silhouette Score (-1 ƒë·∫øn 1)
        """
        n_samples = len(semantic_values)
        
        # Sampling n·∫øu dataset l·ªõn
        if n_samples > sample_size:
            indices = np.random.choice(n_samples, sample_size, replace=False)
            values = semantic_values[indices]
            labels = cluster_labels[indices]
        else:
            values = semantic_values
            labels = cluster_labels
        
        n = len(values)
        unique_labels = np.unique(labels)
        
        if len(unique_labels) < 2:
            return 0.0
        
        # T√≠nh silhouette cho m·ªói sample
        silhouettes = np.zeros(n)
        
        for i in range(n):
            label_i = labels[i]
            
            # a(i): kho·∫£ng c√°ch trung b√¨nh ƒë·∫øn c√°c ƒëi·ªÉm c√πng c·ª•m
            same_cluster = values[labels == label_i]
            if len(same_cluster) > 1:
                a_i = np.mean(np.abs(values[i] - same_cluster))
            else:
                a_i = 0
            
            # b(i): kho·∫£ng c√°ch trung b√¨nh nh·ªè nh·∫•t ƒë·∫øn c·ª•m kh√°c
            b_i = np.inf
            for other_label in unique_labels:
                if other_label != label_i:
                    other_cluster = values[labels == other_label]
                    if len(other_cluster) > 0:
                        dist = np.mean(np.abs(values[i] - other_cluster))
                        b_i = min(b_i, dist)
            
            if b_i == np.inf:
                b_i = 0
            
            # Silhouette
            if max(a_i, b_i) > 0:
                silhouettes[i] = (b_i - a_i) / max(a_i, b_i)
        
        return float(np.mean(silhouettes))
    
    def evaluate(
        self,
        X: np.ndarray,
        cluster_labels: np.ndarray,
        cluster_centers: np.ndarray,
        calculate_silhouette: bool = True
    ) -> ClusteringMetrics:
        """
        T√≠nh t·∫•t c·∫£ c√°c ch·ªâ s·ªë ƒë√°nh gi√° ph√¢n c·ª•m.
        
        ƒê√¢y l√† method ch√≠nh ƒë·ªÉ s·ª≠ d·ª•ng.
        
        Args:
            X: Features array (n_samples, n_features)
            cluster_labels: Nh√£n c·ª•m (n_samples,) - 1-indexed
            cluster_centers: T√¢m c·ª•m (n_clusters,) - semantic values (c√≥ th·ªÉ l√† list)
            calculate_silhouette: C√≥ t√≠nh Silhouette Score kh√¥ng
        
        Returns:
            ClusteringMetrics: K·∫øt qu·∫£ c√°c ch·ªâ s·ªë
        
        Example:
            >>> evaluator = ClusteringEvaluator()
            >>> metrics = evaluator.evaluate(X, labels, centers)
            >>> print(metrics.summary())
        """
        # Convert sang numpy arrays
        X = np.array(X)
        cluster_labels = np.array(cluster_labels)
        cluster_centers = np.array(cluster_centers)
        
        # T√≠nh semantic values
        semantic_values = np.mean(X, axis=1)
        
        # T√≠nh membership matrix
        membership_matrix = self.calculate_membership_matrix(
            semantic_values, cluster_centers
        )
        
        # T√≠nh c√°c metrics
        pc = self.calculate_partition_coefficient(membership_matrix)
        ce = self.calculate_classification_entropy(membership_matrix)
        xb = self.calculate_xie_beni_index(X, membership_matrix, cluster_centers)
        
        # Silhouette (optional)
        silhouette = 0.0
        if calculate_silhouette:
            silhouette = self.calculate_silhouette_score(
                semantic_values, cluster_labels, cluster_centers
            )
        
        return ClusteringMetrics(
            partition_coefficient=pc,
            classification_entropy=ce,
            xie_beni_index=xb,
            silhouette_score=silhouette
        )


def quick_evaluate(
    X: np.ndarray,
    cluster_labels: np.ndarray,
    cluster_centers: np.ndarray
) -> ClusteringMetrics:
    """
    H√†m ti·ªán √≠ch ƒë·ªÉ ƒë√°nh gi√° nhanh.
    
    Args:
        X: Features array
        cluster_labels: Nh√£n c·ª•m
        cluster_centers: T√¢m c·ª•m
    
    Returns:
        ClusteringMetrics
    
    Example:
        >>> metrics = quick_evaluate(X, labels, centers)
        >>> print(f"PC: {metrics.partition_coefficient:.4f}")
    """
    evaluator = ClusteringEvaluator(log_level="ERROR")
    return evaluator.evaluate(X, cluster_labels, cluster_centers)

