"""
Pipeline - Pipeline hoÃ n chá»‰nh tá»« dá»¯ liá»‡u thÃ´ Ä‘áº¿n káº¿t quáº£

ÄÃ¢y lÃ  module chÃ­nh Ä‘á»ƒ sá»­ dá»¥ng, gÃ³i gá»n toÃ n bá»™ quy trÃ¬nh:
1. Load dá»¯ liá»‡u (CSV/NPY)
2. Tiá»n xá»­ lÃ½ (normalize, feature selection)
3. PhÃ¢n cá»¥m (Hedge Algebra)
4. Training model cho tá»«ng cá»¥m
5. Testing vÃ  Ä‘Ã¡nh giÃ¡

Example:
    >>> from src import HedgeAlgebraPipeline
    >>> 
    >>> # CÃ¡ch 1: Sá»­ dá»¥ng vá»›i file CSV
    >>> pipeline = HedgeAlgebraPipeline(n_clusters=3)
    >>> result = pipeline.run("data.csv")
    >>> print(f"Accuracy: {result.accuracy:.4f}")
    >>> 
    >>> # CÃ¡ch 2: Sá»­ dá»¥ng vá»›i dá»¯ liá»‡u Ä‘Ã£ cÃ³
    >>> pipeline = HedgeAlgebraPipeline(n_clusters=3)
    >>> result = pipeline.run_with_data(X_train, X_test, y_train, y_test)
"""

import numpy as np
import time
from typing import Optional, Tuple, Dict, Any, Union, TYPE_CHECKING
from dataclasses import dataclass
from pathlib import Path
from datetime import datetime

from sklearn.base import BaseEstimator
from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier

from .config import ClusteringConfig
from .logger import get_logger
from .data_loader import DataLoader
from .clustering import HedgeAlgebraClustering, ParameterOptimizer
from .classifier import ClusterClassifier, PredictionResult
from .batch_processor import BatchProcessor, MemoryConfig
from .auto_cluster import AutoClusterPipeline, AutoClusterResult
from .cache_utils import CacheManager, CleanConfig, SplitConfig, NormConfig, OptimConfig


@dataclass
class PipelineResult:
    """
    Káº¿t quáº£ cá»§a toÃ n bá»™ pipeline.
    
    Attributes:
        accuracy: Äá»™ chÃ­nh xÃ¡c trÃªn test set
        precision: Precision (macro)
        recall: Recall (macro)
        f1: F1-score (macro)
        training_time: Thá»i gian training (giÃ¢y)
        testing_time: Thá»i gian testing (giÃ¢y)
        n_train_samples: Sá»‘ samples training
        n_test_samples: Sá»‘ samples testing
        n_features: Sá»‘ features
        n_classes: Sá»‘ classes
        n_clusters: Sá»‘ cá»¥m
        cluster_centers: TÃ¢m cÃ¡c cá»¥m
        cluster_distribution: PhÃ¢n bá»‘ samples trong cÃ¡c cá»¥m
        classification_report: BÃ¡o cÃ¡o phÃ¢n loáº¡i chi tiáº¿t
        theta: Tham sá»‘ theta (cÃ³ thá»ƒ Ä‘Ã£ optimize)
        alpha: Tham sá»‘ alpha (cÃ³ thá»ƒ Ä‘Ã£ optimize)
    """
    accuracy: float
    precision: float
    recall: float
    f1: float
    training_time: float
    testing_time: float
    n_train_samples: int
    n_test_samples: int
    n_features: int
    n_classes: int
    n_clusters: int
    cluster_centers: list
    cluster_distribution: dict
    classification_report: str
    theta: float
    alpha: float
    
    def summary(self) -> str:
        """Tráº£ vá» tÃ³m táº¯t káº¿t quáº£."""
        lines = [
            "=" * 60,
            "ðŸ“‹ TÃ“M Táº®T Káº¾T QUáº¢ PIPELINE",
            "=" * 60,
            f"",
            f"ðŸ“Š Dá»® LIá»†U:",
            f"   â€¢ Train samples: {self.n_train_samples:,}",
            f"   â€¢ Test samples: {self.n_test_samples:,}",
            f"   â€¢ Features: {self.n_features:,}",
            f"   â€¢ Classes: {self.n_classes}",
            f"",
            f"ðŸ”§ Cáº¤U HÃŒNH:",
            f"   â€¢ Sá»‘ cá»¥m: {self.n_clusters}",
            f"   â€¢ Theta: {self.theta:.4f}",
            f"   â€¢ Alpha: {self.alpha:.4f}",
            f"   â€¢ TÃ¢m cá»¥m: {[f'{c:.4f}' for c in self.cluster_centers]}",
            f"",
            f"ðŸ“ˆ Káº¾T QUáº¢:",
            f"   â€¢ Accuracy: {self.accuracy:.4f} ({self.accuracy*100:.2f}%)",
            f"   â€¢ Precision: {self.precision:.4f}",
            f"   â€¢ Recall: {self.recall:.4f}",
            f"   â€¢ F1-score: {self.f1:.4f}",
            f"",
            f"â±ï¸ THá»œI GIAN:",
            f"   â€¢ Training: {self.training_time:.2f}s",
            f"   â€¢ Testing: {self.testing_time:.4f}s",
            f"",
            "=" * 60
        ]
        return "\n".join(lines)


class HedgeAlgebraPipeline:
    """
    Pipeline hoÃ n chá»‰nh cho Hedge Algebra Clustering.
    
    ÄÃ¢y lÃ  class chÃ­nh Ä‘á»ƒ sá»­ dá»¥ng module. Há»— trá»£:
    - Load dá»¯ liá»‡u tá»« CSV hoáº·c NPY
    - Tá»± Ä‘á»™ng tiá»n xá»­ lÃ½
    - Tá»‘i Æ°u hÃ³a tham sá»‘ (optional)
    - Training vÃ  testing
    - Logging chi tiáº¿t
    
    Attributes:
        config: Cáº¥u hÃ¬nh pipeline
        data_loader: DataLoader instance
        classifier: ClusterClassifier instance
    
    Example:
        >>> # Sá»­ dá»¥ng cÆ¡ báº£n
        >>> pipeline = HedgeAlgebraPipeline(n_clusters=3)
        >>> result = pipeline.run("data.csv")
        >>> print(result.summary())
        
        >>> # Sá»­ dá»¥ng vá»›i cáº¥u hÃ¬nh tÃ¹y chá»‰nh
        >>> from sklearn.ensemble import RandomForestClassifier
        >>> pipeline = HedgeAlgebraPipeline(
        ...     n_clusters=4,
        ...     theta=0.3,
        ...     alpha=0.4,
        ...     classifier=RandomForestClassifier(n_estimators=100),
        ...     use_information_gain=True,
        ...     optimize_parameters=True
        ... )
        >>> result = pipeline.run("data.csv", label_column="target")
    """
    
    def __init__(
        self,
        n_clusters: int = 2,
        theta: float = 0.5,
        alpha: float = 0.5,
        classifier: Optional[BaseEstimator] = None,
        use_information_gain: bool = False,
        optimize_parameters: bool = False,
        test_size: float = 0.2,
        random_state: int = 42,
        log_level: str = 'INFO',
        log_to_file: bool = True,
        log_dir: str = 'logs',
        center_init: str = 'ver6',
        use_cache: bool = False,
        cache_dir: str = 'cache',
        clean_version: int = 1,
        min_per_class: int = 5
    ):
        """
        Khá»Ÿi táº¡o HedgeAlgebraPipeline.
        
        Args:
            n_clusters: Sá»‘ cá»¥m (2-10)
            theta: Tham sá»‘ theta (0 < theta < 1)
            alpha: Tham sá»‘ alpha (0 < alpha < 1)
            classifier: ML model sklearn (máº·c Ä‘á»‹nh GradientBoostingClassifier)
            use_information_gain: CÃ³ sá»­ dá»¥ng IG weights khÃ´ng
            optimize_parameters: CÃ³ tá»‘i Æ°u theta/alpha khÃ´ng
            test_size: Tá»· lá»‡ test set (0 < test_size < 1)
            random_state: Seed cho reproducibility
            log_level: Má»©c Ä‘á»™ logging (DEBUG, INFO, WARNING, ERROR)
            log_to_file: CÃ³ ghi log ra file khÃ´ng
            log_dir: ThÆ° má»¥c chá»©a file log
        """
        self.n_clusters = n_clusters
        self.theta = theta
        self.alpha = alpha
        self.use_information_gain = use_information_gain
        self.optimize_parameters = optimize_parameters
        self.test_size = test_size
        self.random_state = random_state
        self.center_init = center_init
        self.use_cache = use_cache
        self.cache_dir = cache_dir
        self.clean_version = clean_version
        self.min_per_class = min_per_class
        
        # Classifier
        if classifier is None:
            self.base_classifier = GradientBoostingClassifier(
                random_state=random_state
            )
        else:
            self.base_classifier = classifier
        
        # Logger
        self.logger = get_logger(
            "Pipeline",
            level=log_level,
            log_to_file=log_to_file,
            log_dir=log_dir
        )
        
        # Components (sáº½ Ä‘Æ°á»£c khá»Ÿi táº¡o khi cháº¡y)
        self.data_loader: Optional[DataLoader] = None
        self.classifier: Optional[ClusterClassifier] = None
        self._result: Optional[PipelineResult] = None
    
    def run(
        self,
        file_path: str,
        label_column: Optional[str] = None,
        normalize_method: str = "minmax",
        target_names: Optional[list] = None
    ) -> PipelineResult:
        """
        Cháº¡y pipeline vá»›i file dá»¯ liá»‡u.
        
        ÄÃ¢y lÃ  method chÃ­nh Ä‘á»ƒ sá»­ dá»¥ng pipeline.
        
        Args:
            file_path: ÄÆ°á»ng dáº«n file CSV hoáº·c NPY
            label_column: TÃªn cá»™t label (cho CSV, máº·c Ä‘á»‹nh "label")
            normalize_method: PhÆ°Æ¡ng phÃ¡p chuáº©n hÃ³a ("minmax" hoáº·c "zscore"/"standard")
            target_names: TÃªn cÃ¡c class (optional, Ä‘á»ƒ hiá»ƒn thá»‹)
        
        Returns:
            PipelineResult: Káº¿t quáº£ cá»§a pipeline
        
        Example:
            >>> pipeline = HedgeAlgebraPipeline(n_clusters=3)
            >>> result = pipeline.run("data.csv")
            >>> print(f"Accuracy: {result.accuracy:.4f}")
        """
        self.logger.info("=" * 70)
        self.logger.info("ðŸš€ HEDGE ALGEBRA CLUSTERING PIPELINE")
        self.logger.info(f"   File: {file_path}")
        self.logger.info(f"   Sá»‘ cá»¥m: {self.n_clusters}")
        self.logger.info(f"   Sá»­ dá»¥ng IG: {self.use_information_gain}")
        self.logger.info(f"   Tá»‘i Æ°u tham sá»‘: {self.optimize_parameters}")
        self.logger.info(f"   Sá»­ dá»¥ng cache: {self.use_cache}")
        self.logger.info("=" * 70)
        
        file_path_obj = Path(file_path)
        if not file_path_obj.exists():
            raise FileNotFoundError(f"KhÃ´ng tÃ¬m tháº¥y file: {file_path}")
        
        # Set default label_column
        if label_column is None:
            label_column = "label"
        
        if self.use_cache:
            # --- CACHE FLOW (Level A -> D -> B) ---
            cm = CacheManager(file_path, cache_root=self.cache_dir)
            
            # Level A: Clean
            c_cfg = CleanConfig(
                label_column=label_column,
                clean_version=self.clean_version,
                nan_policy="fill0"
            )
            X_clean, y_raw, clean_h = cm.load_or_build_clean(c_cfg)
            self.logger.info(f"âœ… Level A (Clean): {X_clean.shape[0]} samples, {X_clean.shape[1]} features")
            
            # Level D: Split (Fair Comparison Key)
            s_cfg = SplitConfig(
                test_size=self.test_size,
                random_state=self.random_state,
                min_per_class=self.min_per_class
            )
            train_idx, test_idx, split_h = cm.load_or_build_split(y_raw, clean_h, s_cfg)
            self.logger.info(f"âœ… Level D (Split): {len(train_idx)} train, {len(test_idx)} test")
            
            # Level B: Normalize
            # Map method name tá»« pipeline sang cache config
            norm_method_map = "zscore" if normalize_method in ("zscore", "standard") else "minmax"
            n_cfg = NormConfig(method=norm_method_map)
            
            # HÃ m load_or_build_norm tráº£ vá» (payload, norm_h)
            data_norm, norm_h = cm.load_or_build_norm(X_clean, y_raw, train_idx, test_idx, split_h, n_cfg)
            
            X_train, X_test = data_norm["X_train"], data_norm["X_test"]
            y_train, y_test = data_norm["y_train"], data_norm["y_test"]
            
            self.logger.info(f"âœ… Level B (Normalize): Loaded from cache. Train shape: {X_train.shape}, Test shape: {X_test.shape}")
            
            # TÃ­nh IG weights náº¿u cáº§n (chÆ°a cÃ³ trong cache, tÃ­nh láº¡i)
            ig_weights = None
            if self.use_information_gain:
                self.data_loader = DataLoader(log_level="INFO")
                ig_weights = self.data_loader.calculate_information_gain_ratio(X_train, y_train)
                self.logger.info("âœ… Information Gain Ratio calculated")
        else:
            # Fallback: Logic cÅ© khÃ´ng dÃ¹ng cache
            self.data_loader = DataLoader(log_level="INFO")
            
            X_train, X_test, y_train, y_test, ig_weights = self.data_loader.load_and_preprocess(
                file_path=file_path,
                label_column=label_column,
                normalize_method=normalize_method,
                remove_constant=True,
                calculate_ig=self.use_information_gain,
                test_size=self.test_size,
                random_state=self.random_state
            )
            norm_h = None  # KhÃ´ng cÃ³ hash khi khÃ´ng dÃ¹ng cache
        
        # --- Level E Hook (Optimization) ---
        if self.optimize_parameters:
            if self.use_cache:
                o_cfg = OptimConfig(
                    center_init=self.center_init,
                    n_clusters=self.n_clusters,
                    theta_range=(0.01, 0.5, 0.01),
                    alpha_range=(0.01, 0.5, 0.01)
                )
                
                # Wrapper function Ä‘á»ƒ gá»i optimizer cÅ©
                def _run_optim(X, cfg):
                    opt = ParameterOptimizer(
                        center_init=cfg.center_init,
                        theta_range=cfg.theta_range,
                        alpha_range=cfg.alpha_range,
                        log_level=self.log_level
                    )
                    # ParameterOptimizer.optimize tráº£ vá» (theta, alpha, min_distance)
                    return opt.optimize(X, cfg.n_clusters)
                
                self.theta, self.alpha, best_score = cm.load_or_build_best_params(
                    X_train, norm_h, o_cfg, _run_optim
                )
                self.logger.info(f"âœ… Level E (Optimization): theta={self.theta:.4f}, alpha={self.alpha:.4f} (Score: {best_score:.4f})")
            else:
                # Logic cÅ©: tá»‘i Æ°u trá»±c tiáº¿p
                self.logger.info("\nðŸ“ Tá»‘i Æ°u hÃ³a tham sá»‘ theta vÃ  alpha")
                optimizer = ParameterOptimizer(log_level="INFO", center_init=self.center_init)
                self.theta, self.alpha, min_distance = optimizer.optimize(X_train, self.n_clusters)
                self.logger.info(f"   âœ… Tá»‘i Æ°u: Î¸={self.theta:.4f}, Î±={self.alpha:.4f}, Distance={min_distance:.4f}")
        
        # Cháº¡y vá»›i dá»¯ liá»‡u Ä‘Ã£ load
        return self.run_with_data(
            X_train=X_train,
            X_test=X_test,
            y_train=y_train,
            y_test=y_test,
            information_gain_weights=ig_weights
        )
    
    def run_with_data(
        self,
        X_train: np.ndarray,
        X_test: np.ndarray,
        y_train: np.ndarray,
        y_test: np.ndarray,
        information_gain_weights: Optional[np.ndarray] = None
    ) -> PipelineResult:
        """
        Cháº¡y pipeline vá»›i dá»¯ liá»‡u Ä‘Ã£ cÃ³ sáºµn.
        
        Sá»­ dá»¥ng khi Ä‘Ã£ cÃ³ dá»¯ liá»‡u train/test sáºµn.
        
        Args:
            X_train: Features training
            X_test: Features testing
            y_train: Labels training
            y_test: Labels testing
            information_gain_weights: Trá»ng sá»‘ IG (optional)
        
        Returns:
            PipelineResult: Káº¿t quáº£ cá»§a pipeline
        
        Example:
            >>> pipeline = HedgeAlgebraPipeline(n_clusters=3)
            >>> result = pipeline.run_with_data(X_train, X_test, y_train, y_test)
        """
        pipeline_start_time = time.time()
        
        n_train_samples, n_features = X_train.shape
        n_test_samples = X_test.shape[0]
        unique_classes = np.unique(y_train)
        n_classes = len(unique_classes)
        
        theta = self.theta
        alpha = self.alpha
        
        # 2. Tá»‘i Æ°u tham sá»‘ (náº¿u cáº§n)
        if self.optimize_parameters:
            self.logger.info("\nðŸ“ Tá»‘i Æ°u hÃ³a tham sá»‘ theta vÃ  alpha")
            optimizer = ParameterOptimizer(log_level="INFO", center_init=self.center_init)
            theta, alpha, min_distance = optimizer.optimize(
                X_train, self.n_clusters
            )
            self.theta = theta
            self.alpha = alpha
        
        # 3. Khá»Ÿi táº¡o vÃ  train classifier
        self.classifier = ClusterClassifier(
            n_clusters=self.n_clusters,
            theta=theta,
            alpha=alpha,
            base_classifier=self.base_classifier,
            use_information_gain=self.use_information_gain,
            random_state=self.random_state,
            log_level="INFO",
            center_init=self.center_init
        )
        
        self.classifier.fit(
            X_train, y_train,
            information_gain_weights=information_gain_weights
        )
        
        training_time = self.classifier.training_time
        
        # 4. Test vÃ  Ä‘Ã¡nh giÃ¡
        prediction_result = self.classifier.predict(X_test, y_test)
        
        # 5. TÃ­nh phÃ¢n bá»‘ cá»¥m
        cluster_distribution = {}
        clustering_result = self.classifier._clustering_result
        if clustering_result is not None:
            for cluster_id in range(1, self.n_clusters + 1):
                count = int(np.sum(clustering_result.cluster_labels == cluster_id))
                cluster_distribution[cluster_id] = count
        
        # 6. Táº¡o káº¿t quáº£
        self._result = PipelineResult(
            accuracy=prediction_result.accuracy,
            precision=prediction_result.precision,
            recall=prediction_result.recall,
            f1=prediction_result.f1,
            training_time=training_time,
            testing_time=prediction_result.total_time,
            n_train_samples=n_train_samples,
            n_test_samples=n_test_samples,
            n_features=n_features,
            n_classes=n_classes,
            n_clusters=self.n_clusters,
            cluster_centers=self.classifier.cluster_centers,
            cluster_distribution=cluster_distribution,
            classification_report=prediction_result.classification_report,
            theta=theta,
            alpha=alpha
        )
        
        total_time = time.time() - pipeline_start_time
        
        # Log summary
        self.logger.info("\n" + self._result.summary())
        self.logger.info(f"\nâ±ï¸ Tá»•ng thá»i gian pipeline: {total_time:.2f}s")
        
        return self._result
    
    def get_predictions(self, X: np.ndarray) -> np.ndarray:
        """
        Láº¥y predictions cho dá»¯ liá»‡u má»›i (khÃ´ng cÃ³ labels).
        
        Args:
            X: Features
        
        Returns:
            np.ndarray: Predictions
        """
        if self.classifier is None or not self.classifier._is_fitted:
            raise ValueError("Pipeline chÆ°a Ä‘Æ°á»£c cháº¡y. Gá»i run() trÆ°á»›c.")
        
        result = self.classifier.predict(X)
        return result.predictions
    
    def save(self, directory: str = "saved_pipeline"):
        """
        LÆ°u pipeline Ä‘Ã£ train.
        
        Args:
            directory: ThÆ° má»¥c lÆ°u
        """
        if self.classifier is None:
            raise ValueError("Pipeline chÆ°a Ä‘Æ°á»£c cháº¡y.")
        
        self.logger.info(f"\nðŸ’¾ LÆ°u pipeline vÃ o: {directory}")
        self.classifier.save_models(directory)
    
    def load(self, directory: str = "saved_pipeline"):
        """
        Load pipeline Ä‘Ã£ lÆ°u.
        
        Args:
            directory: ThÆ° má»¥c chá»©a pipeline
        """
        self.logger.info(f"\nðŸ“‚ Load pipeline tá»«: {directory}")
        
        self.classifier = ClusterClassifier(log_level="INFO", center_init=self.center_init)
        self.classifier.load_models(directory)
        
        # Cáº­p nháº­t config tá»« metadata
        self.n_clusters = self.classifier.n_clusters
        self.theta = self.classifier.theta
        self.alpha = self.classifier.alpha
    
    @property
    def result(self) -> Optional[PipelineResult]:
        """Tráº£ vá» káº¿t quáº£ pipeline (náº¿u Ä‘Ã£ cháº¡y)."""
        return self._result
    
    def run_auto_cluster(
        self,
        X_train: np.ndarray,
        X_test: np.ndarray,
        y_train: np.ndarray,
        y_test: np.ndarray,
        min_clusters: int = 2,
        max_clusters: int = 9,
        selection_metric: str = "silhouette",
        information_gain_weights: Optional[np.ndarray] = None
    ) -> Tuple['PipelineResult', 'AutoClusterResult']:
        """
        Tá»± Ä‘á»™ng cháº¡y tá»« min_clusters Ä‘áº¿n max_clusters vÃ  chá»n cá»¥m tá»‘t nháº¥t.
        
        Args:
            X_train: Features training
            X_test: Features testing
            y_train: Labels training
            y_test: Labels testing
            min_clusters: Sá»‘ cá»¥m tá»‘i thiá»ƒu (máº·c Ä‘á»‹nh 2)
            max_clusters: Sá»‘ cá»¥m tá»‘i Ä‘a (máº·c Ä‘á»‹nh 9)
            selection_metric: Metric Ä‘á»ƒ chá»n cá»¥m ("silhouette", "distance", "elbow")
            information_gain_weights: Trá»ng sá»‘ IG (optional)
        
        Returns:
            Tuple[PipelineResult, AutoClusterResult]: Káº¿t quáº£ pipeline vÃ  auto cluster
        
        Example:
            >>> pipeline = HedgeAlgebraPipeline()
            >>> result, auto_result = pipeline.run_auto_cluster(
            ...     X_train, X_test, y_train, y_test,
            ...     min_clusters=2, max_clusters=9
            ... )
            >>> print(f"Best clusters: {auto_result.best_n_clusters}")
            >>> print(f"Accuracy: {result.accuracy:.4f}")
        """
        self.logger.info("=" * 70)
        self.logger.info("ðŸ”„ AUTO CLUSTER PIPELINE")
        self.logger.info(f"   TÃ¬m sá»‘ cá»¥m tá»‘t nháº¥t tá»« {min_clusters} Ä‘áº¿n {max_clusters}")
        self.logger.info("=" * 70)
        
        # BÆ°á»›c 1: Cháº¡y auto cluster Ä‘á»ƒ tÃ¬m sá»‘ cá»¥m tá»‘t nháº¥t
        auto_pipeline = AutoClusterPipeline(
            min_clusters=min_clusters,
            max_clusters=max_clusters,
            optimize_params=self.optimize_parameters,
            log_level="INFO",
            center_init=self.center_init
        )
        
        auto_result = auto_pipeline.run(X_train, selection_metric=selection_metric)
        
        # BÆ°á»›c 2: Cáº­p nháº­t config vá»›i cá»¥m tá»‘t nháº¥t
        best_n_clusters = auto_result.best_n_clusters
        best_theta = auto_result.best_evaluation.theta
        best_alpha = auto_result.best_evaluation.alpha
        
        self.n_clusters = best_n_clusters
        self.theta = best_theta
        self.alpha = best_alpha
        
        self.logger.info(f"\nðŸ† Sá»‘ cá»¥m tá»‘t nháº¥t: {best_n_clusters}")
        self.logger.info(f"   Î¸ = {best_theta:.4f}, Î± = {best_alpha:.4f}")
        
        # BÆ°á»›c 3: Cháº¡y pipeline vá»›i cáº¥u hÃ¬nh tá»‘t nháº¥t
        self.logger.info("\nðŸ“ Training vá»›i cáº¥u hÃ¬nh tá»‘i Æ°u...")
        
        result = self.run_with_data(
            X_train, X_test, y_train, y_test,
            information_gain_weights=information_gain_weights
        )
        
        return result, auto_result


def quick_run(
    file_path: str,
    n_clusters: int = 2,
    label_column: Optional[str] = None,
    optimize: bool = False
) -> PipelineResult:
    """
    HÃ m tiá»‡n Ã­ch Ä‘á»ƒ cháº¡y nhanh pipeline.
    
    ÄÃ¢y lÃ  cÃ¡ch Ä‘Æ¡n giáº£n nháº¥t Ä‘á»ƒ sá»­ dá»¥ng module.
    
    Args:
        file_path: ÄÆ°á»ng dáº«n file CSV hoáº·c NPY
        n_clusters: Sá»‘ cá»¥m
        label_column: TÃªn cá»™t label (optional)
        optimize: CÃ³ tá»‘i Æ°u tham sá»‘ khÃ´ng
    
    Returns:
        PipelineResult: Káº¿t quáº£
    
    Example:
        >>> from src.pipeline import quick_run
        >>> result = quick_run("data.csv", n_clusters=3)
        >>> print(f"Accuracy: {result.accuracy:.4f}")
    """
    pipeline = HedgeAlgebraPipeline(
        n_clusters=n_clusters,
        optimize_parameters=optimize
    )
    return pipeline.run(file_path, label_column=label_column)


def quick_auto_run(
    file_path: str,
    min_clusters: int = 2,
    max_clusters: int = 9,
    label_column: Optional[str] = None,
    max_memory_gb: float = 4.0
) -> Tuple[PipelineResult, AutoClusterResult]:
    """
    HÃ m tiá»‡n Ã­ch Ä‘á»ƒ cháº¡y auto cluster nhanh.
    
    Tá»± Ä‘á»™ng tÃ¬m sá»‘ cá»¥m tá»‘t nháº¥t tá»« min_clusters Ä‘áº¿n max_clusters.
    
    Args:
        file_path: ÄÆ°á»ng dáº«n file CSV hoáº·c NPY
        min_clusters: Sá»‘ cá»¥m tá»‘i thiá»ƒu
        max_clusters: Sá»‘ cá»¥m tá»‘i Ä‘a
        label_column: TÃªn cá»™t label (optional)
        max_memory_gb: RAM tá»‘i Ä‘a (GB)
    
    Returns:
        Tuple[PipelineResult, AutoClusterResult]
    
    Example:
        >>> from src.pipeline import quick_auto_run
        >>> result, auto_result = quick_auto_run("data.csv", min_clusters=2, max_clusters=9)
        >>> print(f"Best: {auto_result.best_n_clusters} clusters")
        >>> print(f"Accuracy: {result.accuracy:.4f}")
    """
    # Load dá»¯ liá»‡u
    data_loader = DataLoader(log_level="INFO")
    X_train, X_test, y_train, y_test, ig_weights = data_loader.load_and_preprocess(
        file_path=file_path,
        label_column=label_column,
        normalize_method="minmax",
        calculate_ig=False
    )
    
    # Cháº¡y auto cluster
    pipeline = HedgeAlgebraPipeline(
        n_clusters=2,  # Sáº½ Ä‘Æ°á»£c cáº­p nháº­t bá»Ÿi auto cluster
        log_level="INFO",
        log_to_file=False
    )
    
    return pipeline.run_auto_cluster(
        X_train, X_test, y_train, y_test,
        min_clusters=min_clusters,
        max_clusters=max_clusters
    )


class LargeDatasetPipeline:
    """
    Pipeline cho dataset lá»›n (hÃ ng triá»‡u dÃ²ng).
    
    Sá»­ dá»¥ng batch processing Ä‘á»ƒ trÃ¡nh trÃ n RAM.
    
    Example:
        >>> pipeline = LargeDatasetPipeline(max_memory_gb=8.0)
        >>> result = pipeline.run("large_data.csv", n_clusters=5)
    """
    
    def __init__(
        self,
        max_memory_gb: float = 4.0,
        batch_size: Optional[int] = None,
        log_level: str = "INFO"
    ):
        """
        Khá»Ÿi táº¡o LargeDatasetPipeline.
        
        Args:
            max_memory_gb: RAM tá»‘i Ä‘a Ä‘Æ°á»£c sá»­ dá»¥ng (GB)
            batch_size: KÃ­ch thÆ°á»›c batch (None = tá»± Ä‘á»™ng)
            log_level: Má»©c Ä‘á»™ logging
        """
        self.max_memory_gb = max_memory_gb
        self.batch_size = batch_size
        self.logger = get_logger("LargeDataPipeline", level=log_level, log_to_file=False)
        self.batch_processor = BatchProcessor(
            max_memory_gb=max_memory_gb,
            batch_size=batch_size,
            log_level=log_level
        )
    
    def run(
        self,
        file_path: str,
        n_clusters: int = 2,
        label_column: Optional[str] = None,
        sample_for_training: int = 100000
    ) -> PipelineResult:
        """
        Cháº¡y pipeline cho dataset lá»›n.
        
        Sá»­ dá»¥ng sampling cho training vÃ  batch processing cho prediction.
        
        Args:
            file_path: ÄÆ°á»ng dáº«n file
            n_clusters: Sá»‘ cá»¥m
            label_column: TÃªn cá»™t label
            sample_for_training: Sá»‘ samples dÃ¹ng Ä‘á»ƒ train
        
        Returns:
            PipelineResult
        """
        self.logger.info("=" * 70)
        self.logger.info("ðŸš€ LARGE DATASET PIPELINE")
        self.logger.info(f"   Max RAM: {self.max_memory_gb} GB")
        self.logger.info(f"   Sample for training: {sample_for_training:,}")
        self.logger.info("=" * 70)
        
        # Load vÃ  sample dá»¯ liá»‡u
        data_loader = DataLoader(log_level="INFO")
        X_train, X_test, y_train, y_test, _ = data_loader.load_and_preprocess(
            file_path=file_path,
            label_column=label_column,
            normalize_method="minmax"
        )
        
        # Sample náº¿u dataset quÃ¡ lá»›n
        n_train = X_train.shape[0]
        if n_train > sample_for_training:
            self.logger.info(f"ðŸ“¦ Sampling {sample_for_training:,} tá»« {n_train:,} samples")
            indices = np.random.choice(n_train, sample_for_training, replace=False)
            X_train_sample = X_train[indices]
            y_train_sample = y_train[indices]
        else:
            X_train_sample = X_train
            y_train_sample = y_train
        
        # Cháº¡y pipeline
        pipeline = HedgeAlgebraPipeline(
            n_clusters=n_clusters,
            log_level="INFO",
            log_to_file=False
        )
        
        return pipeline.run_with_data(
            X_train_sample, X_test,
            y_train_sample, y_test
        )

