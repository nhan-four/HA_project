"""
Auto Cluster - T·ª± ƒë·ªông ch·∫°y v√† ch·ªçn s·ªë c·ª•m t·ªëi ∆∞u (2-9)

Features:
- Ch·∫°y song song nhi·ªÅu s·ªë c·ª•m
- ƒê√°nh gi√° v√† ch·ªçn c·ª•m t·ªët nh·∫•t
- T·ªëi ∆∞u h√≥a v·ªõi numpy vectorization
- Batch processing cho dataset l·ªõn
"""

import numpy as np
from typing import List, Tuple, Optional, Dict
from dataclasses import dataclass
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
import time

from .config import MIN_CLUSTERS, MAX_CLUSTERS, MIN_VALUE, MAX_VALUE
from .logger import get_logger
from .clustering import HedgeAlgebraClustering, ClusteringResult, ParameterOptimizer
from .batch_processor import BatchProcessor, MemoryConfig
from .clustering_metrics import ClusteringEvaluator, ClusteringMetrics


@dataclass
class ClusterEvaluation:
    """
    K·∫øt qu·∫£ ƒë√°nh gi√° c·ªßa m·ªôt c·∫•u h√¨nh c·ª•m.
    
    Attributes:
        n_clusters: S·ªë c·ª•m
        theta: Tham s·ªë theta
        alpha: Tham s·ªë alpha
        accuracy: ƒê·ªô ch√≠nh x√°c (n·∫øu c√≥ labels)
        silhouette_score: Silhouette score
        total_distance: T·ªïng kho·∫£ng c√°ch ƒë·∫øn t√¢m c·ª•m
        cluster_distribution: Ph√¢n b·ªë samples trong c√°c c·ª•m
        training_time: Th·ªùi gian training (s)
        partition_coefficient: Partition Coefficient (PC)
        classification_entropy: Classification Entropy (CE)
        xie_beni_index: Xie-Beni Index (XB)
    """
    n_clusters: int
    theta: float
    alpha: float
    accuracy: float = 0.0
    silhouette_score: float = 0.0
    total_distance: float = 0.0
    cluster_distribution: Dict[int, int] = None
    training_time: float = 0.0
    partition_coefficient: float = 0.0
    classification_entropy: float = 0.0
    xie_beni_index: float = 0.0


@dataclass
class AutoClusterResult:
    """
    K·∫øt qu·∫£ c·ªßa AutoCluster.
    
    Attributes:
        best_n_clusters: S·ªë c·ª•m t·ªët nh·∫•t
        best_evaluation: ƒê√°nh gi√° c·ªßa c·∫•u h√¨nh t·ªët nh·∫•t
        all_evaluations: Danh s√°ch ƒë√°nh gi√° c·ªßa t·∫•t c·∫£ c√°c c·∫•u h√¨nh
        total_time: T·ªïng th·ªùi gian ch·∫°y
    """
    best_n_clusters: int
    best_evaluation: ClusterEvaluation
    all_evaluations: List[ClusterEvaluation]
    total_time: float
    
    def summary(self) -> str:
        """T·∫°o b·∫£ng t√≥m t·∫Øt k·∫øt qu·∫£."""
        lines = [
            "=" * 100,
            "üìä K·∫æT QU·∫¢ AUTO CLUSTER",
            "=" * 100,
            "",
            f"üèÜ S·ªë c·ª•m t·ªët nh·∫•t: {self.best_n_clusters}",
            f"   Œ∏ = {self.best_evaluation.theta:.4f}, Œ± = {self.best_evaluation.alpha:.4f}",
            f"   Silhouette: {self.best_evaluation.silhouette_score:.4f}",
            f"   Total Distance: {self.best_evaluation.total_distance:.4f}",
            f"   PC: {self.best_evaluation.partition_coefficient:.4f}",
            f"   CE: {self.best_evaluation.classification_entropy:.4f}",
            f"   XB: {self.best_evaluation.xie_beni_index:.4f}",
            "",
            "üìã Chi ti·∫øt t·∫•t c·∫£ c√°c c·∫•u h√¨nh:",
            "-" * 100,
            f"{'N':>3} | {'Theta':>8} | {'Alpha':>8} | {'Silhouette':>12} | {'PC':>10} | {'CE':>10} | {'XB':>12} | {'Time(s)':>8}",
            "-" * 100
        ]
        
        for eval in sorted(self.all_evaluations, key=lambda x: x.n_clusters):
            lines.append(
                f"{eval.n_clusters:>3} | {eval.theta:>8.4f} | {eval.alpha:>8.4f} | "
                f"{eval.silhouette_score:>12.4f} | {eval.partition_coefficient:>10.4f} | "
                f"{eval.classification_entropy:>10.4f} | {eval.xie_beni_index:>12.4f} | "
                f"{eval.training_time:>8.2f}"
            )
        
        lines.extend([
            "-" * 100,
            f"‚è±Ô∏è T·ªïng th·ªùi gian: {self.total_time:.2f}s",
            "=" * 100
        ])
        
        return "\n".join(lines)


class OptimizedClustering:
    """
    Phi√™n b·∫£n t·ªëi ∆∞u h√≥a c·ªßa HedgeAlgebraClustering.
    
    S·ª≠ d·ª•ng numpy vectorization ƒë·ªÉ tƒÉng t·ªëc ƒë·ªô x·ª≠ l√Ω.
    """
    
    @staticmethod
    def calculate_semantic_values_vectorized(X: np.ndarray) -> np.ndarray:
        """T√≠nh semantic values v·ªõi vectorization (nhanh h∆°n)."""
        return np.mean(X, axis=1)
    
    @staticmethod
    def assign_to_clusters_vectorized(
        semantic_values: np.ndarray,
        cluster_centers: np.ndarray
    ) -> np.ndarray:
        """
        G√°n ƒëi·ªÉm v√†o c·ª•m v·ªõi vectorization (nhanh h∆°n nhi·ªÅu l·∫ßn).
        
        S·ª≠ d·ª•ng broadcasting thay v√¨ v√≤ng l·∫∑p.
        """
        centers = np.array(cluster_centers)
        n_clusters = len(centers)
        
        # T√≠nh boundaries gi·ªØa c√°c c·ª•m
        # boundary[i] = (centers[i] + centers[i+1]) / 2
        boundaries = (centers[:-1] + centers[1:]) / 2
        
        # S·ª≠ d·ª•ng searchsorted ƒë·ªÉ t√¨m c·ª•m nhanh
        # searchsorted tr·∫£ v·ªÅ index n∆°i gi√° tr·ªã n√™n ƒë∆∞·ª£c ch√®n v√†o
        cluster_labels = np.searchsorted(boundaries, semantic_values, side='left') + 1

        return cluster_labels.astype(np.int32)
    
    @staticmethod
    def update_centers_vectorized(
        semantic_values: np.ndarray,
        cluster_labels: np.ndarray,
        n_clusters: int,
        old_centers: np.ndarray
    ) -> np.ndarray:
        """C·∫≠p nh·∫≠t t√¢m c·ª•m. N·∫øu c·ª•m r·ªóng -> gi·ªØ t√¢m c≈© (tr√°nh teleport)."""
        old_centers = np.asarray(old_centers, dtype=float)
        new_centers = np.empty(n_clusters, dtype=float)

        for i in range(n_clusters):
            cluster_id = i + 1
            mask = (cluster_labels == cluster_id)
            if np.any(mask):
                new_centers[i] = float(np.mean(semantic_values[mask]))
            else:
                new_centers[i] = float(old_centers[i])

        new_centers = np.clip(new_centers, MIN_VALUE, MAX_VALUE)
        return np.sort(new_centers)

    @staticmethod
    def calculate_total_distance_vectorized(
        semantic_values: np.ndarray,
        cluster_labels: np.ndarray,
        cluster_centers: np.ndarray
    ) -> float:
        """T√≠nh t·ªïng kho·∫£ng c√°ch (L2 Squared) ƒë·ªìng b·ªô v·ªõi core Ver 6.5."""
        centers = np.asarray(cluster_centers, dtype=float)
        label_indices = (cluster_labels - 1).astype(int)
        valid_mask = (label_indices >= 0) & (label_indices < len(centers))
        if not np.any(valid_mask):
            return 0.0
        sample_centers = centers[label_indices[valid_mask]]
        diffs = semantic_values[valid_mask] - sample_centers
        return float(np.sum(diffs ** 2))

    @staticmethod
    def calculate_silhouette_fast(
        semantic_values: np.ndarray,
        cluster_labels: np.ndarray,
        cluster_centers: np.ndarray,
        sample_size: int = 5000
    ) -> float:
        """
        T√≠nh Silhouette score nhanh (approximate).
        
        S·ª≠ d·ª•ng sampling n·∫øu dataset l·ªõn ƒë·ªÉ tƒÉng t·ªëc.
        """
        n_samples = len(semantic_values)
        
        # Sampling n·∫øu dataset l·ªõn
        if n_samples > sample_size:
            indices = np.random.choice(n_samples, sample_size, replace=False)
            values = semantic_values[indices]
            labels = cluster_labels[indices]
        else:
            values = semantic_values
            labels = cluster_labels
        
        n = len(values)
        unique_labels = np.unique(labels)
        
        if len(unique_labels) < 2:
            return 0.0
        
        # T√≠nh silhouette cho m·ªói sample
        silhouettes = np.zeros(n)
        
        for i in range(n):
            label_i = labels[i]
            
            # a(i): kho·∫£ng c√°ch trung b√¨nh ƒë·∫øn c√°c ƒëi·ªÉm c√πng c·ª•m
            same_cluster = values[labels == label_i]
            if len(same_cluster) > 1:
                a_i = np.mean(np.abs(values[i] - same_cluster))
            else:
                a_i = 0
            
            # b(i): kho·∫£ng c√°ch trung b√¨nh nh·ªè nh·∫•t ƒë·∫øn c·ª•m kh√°c
            b_i = np.inf
            for other_label in unique_labels:
                if other_label != label_i:
                    other_cluster = values[labels == other_label]
                    if len(other_cluster) > 0:
                        dist = np.mean(np.abs(values[i] - other_cluster))
                        b_i = min(b_i, dist)
            
            if b_i == np.inf:
                b_i = 0
            
            # Silhouette
            if max(a_i, b_i) > 0:
                silhouettes[i] = (b_i - a_i) / max(a_i, b_i)
        
        return np.mean(silhouettes)


class AutoClusterPipeline:
    """
    Pipeline t·ª± ƒë·ªông ch·∫°y v√† ƒë√°nh gi√° nhi·ªÅu s·ªë c·ª•m.
    
    Ch·∫°y t·ª´ 2-9 c·ª•m v√† ch·ªçn c·∫•u h√¨nh t·ªët nh·∫•t d·ª±a tr√™n
    Silhouette score ho·∫∑c t·ªïng kho·∫£ng c√°ch.
    
    Example:
        >>> auto = AutoClusterPipeline(min_clusters=2, max_clusters=9)
        >>> result = auto.run(X)
        >>> print(result.summary())
        >>> print(f"Best: {result.best_n_clusters} clusters")
    """
    
    def __init__(
        self,
        min_clusters: int = 2,
        max_clusters: int = 9,
        optimize_params: bool = True,
        max_memory_gb: float = 4.0,
        n_jobs: int = 1,
        log_level: str = "INFO",
        center_init: str = "ver6"
    ):
        """
        Kh·ªüi t·∫°o AutoClusterPipeline.
        
        Args:
            min_clusters: S·ªë c·ª•m t·ªëi thi·ªÉu (>= 2)
            max_clusters: S·ªë c·ª•m t·ªëi ƒëa (<= 10)
            optimize_params: C√≥ t·ªëi ∆∞u theta/alpha cho m·ªói c·∫•u h√¨nh kh√¥ng
            max_memory_gb: RAM t·ªëi ƒëa (GB)
            n_jobs: S·ªë jobs ch·∫°y song song (1 = sequential)
            log_level: M·ª©c ƒë·ªô logging
        """
        self.min_clusters = max(MIN_CLUSTERS, min_clusters)
        self.max_clusters = min(MAX_CLUSTERS, max_clusters)
        self.optimize_params = optimize_params
        self.max_memory_gb = max_memory_gb
        self.n_jobs = n_jobs

        self.center_init = center_init
        if self.center_init not in ("ver6", "legacy"):
            raise ValueError(f"center_init ph·∫£i l√† 'ver6' ho·∫∑c 'legacy', nh·∫≠n ƒë∆∞·ª£c: {self.center_init}")
        self.logger = get_logger("AutoCluster", level=log_level, log_to_file=False)
        self.batch_processor = BatchProcessor(max_memory_gb=max_memory_gb, log_level="WARNING")
    
    def _evaluate_single_config(
        self,
        X: np.ndarray,
        n_clusters: int,
        theta: float = None,
        alpha: float = None
    ) -> ClusterEvaluation:
        """
        ƒê√°nh gi√° m·ªôt c·∫•u h√¨nh c·ª•m.
        
        Args:
            X: Features
            n_clusters: S·ªë c·ª•m
            theta: Tham s·ªë theta (None = t·ªëi ∆∞u)
            alpha: Tham s·ªë alpha (None = t·ªëi ∆∞u)
        
        Returns:
            ClusterEvaluation: K·∫øt qu·∫£ ƒë√°nh gi√°
        """
        start_time = time.time()
        
        # T√≠nh semantic values
        semantic_values = OptimizedClustering.calculate_semantic_values_vectorized(X)
        
        # T·ªëi ∆∞u theta/alpha n·∫øu c·∫ßn
        if self.optimize_params and (theta is None or alpha is None):
            optimizer = ParameterOptimizer(
                theta_range=(0.1, 0.5, 0.05),
                alpha_range=(0.1, 0.5, 0.05),
                log_level="ERROR",
                center_init=self.center_init
            )
            theta, alpha, _ = optimizer.optimize(X, n_clusters)
        else:
            theta = theta or 0.5
            alpha = alpha or 0.5
        
        # Kh·ªüi t·∫°o centers
        clustering = HedgeAlgebraClustering(
            n_clusters=n_clusters,
            theta=theta,
            alpha=alpha,
            log_level="ERROR",
            center_init=self.center_init
        )
        centers = np.array(clustering.initialize_cluster_centers())

        # Semantic Scaling (sync with core): map centers [0,1] into [min(Sd), max(Sd)]
        min_sd = float(np.min(semantic_values))
        max_sd = float(np.max(semantic_values))
        range_sd = max_sd - min_sd
        if range_sd > 1e-6:
            centers = min_sd + centers * range_sd
        
        # Clustering iterations
        for _ in range(50):  # Max iterations
            labels = OptimizedClustering.assign_to_clusters_vectorized(semantic_values, centers)
            new_centers = OptimizedClustering.update_centers_vectorized(semantic_values, labels, n_clusters, centers)
            
            if np.allclose(centers, new_centers, atol=1e-6):
                break
            centers = new_centers
        
        # T√≠nh metrics
        total_distance = OptimizedClustering.calculate_total_distance_vectorized(
            semantic_values, labels, centers
        )
        silhouette = OptimizedClustering.calculate_silhouette_fast(
            semantic_values, labels, centers
        )
        
        # T√≠nh clustering metrics (PC, CE, XB)
        try:
            evaluator = ClusteringEvaluator(log_level="ERROR")
            metrics = evaluator.evaluate(X, labels, centers, calculate_silhouette=False)
            pc = metrics.partition_coefficient
            ce = metrics.classification_entropy
            xb = metrics.xie_beni_index
        except Exception:
            pc = ce = xb = 0.0
        
        # Ph√¢n b·ªë c·ª•m
        distribution = {}
        for k in range(1, n_clusters + 1):
            distribution[k] = int(np.sum(labels == k))
        
        training_time = time.time() - start_time
        
        return ClusterEvaluation(
            n_clusters=n_clusters,
            theta=theta,
            alpha=alpha,
            silhouette_score=silhouette,
            total_distance=total_distance,
            cluster_distribution=distribution,
            training_time=training_time,
            partition_coefficient=pc,
            classification_entropy=ce,
            xie_beni_index=xb
        )
    
    def run(
        self,
        X: np.ndarray,
        selection_metric: str = "silhouette"
    ) -> AutoClusterResult:
        """
        Ch·∫°y auto clustering v√† ch·ªçn c·∫•u h√¨nh t·ªët nh·∫•t.
        
        Args:
            X: Features array
            selection_metric: Metric ƒë·ªÉ ch·ªçn c·ª•m t·ªët nh·∫•t
                - "silhouette": Silhouette score cao nh·∫•t
                - "distance": T·ªïng kho·∫£ng c√°ch th·∫•p nh·∫•t
                - "elbow": Ph∆∞∆°ng ph√°p elbow
        
        Returns:
            AutoClusterResult: K·∫øt qu·∫£ v·ªõi c·ª•m t·ªët nh·∫•t
        """
        start_time = time.time()
        
        self.logger.info("=" * 70)
        self.logger.info("üîÑ B·∫ÆT ƒê·∫¶U AUTO CLUSTER")
        self.logger.info(f"   S·ªë c·ª•m: {self.min_clusters} ‚Üí {self.max_clusters}")
        self.logger.info(f"   T·ªëi ∆∞u params: {self.optimize_params}")
        self.logger.info(f"   Selection metric: {selection_metric}")
        self.logger.info("=" * 70)
        
        evaluations = []
        cluster_range = range(self.min_clusters, self.max_clusters + 1)
        
        for n_clusters in cluster_range:
            self.logger.info(f"\nüìç ƒêang ƒë√°nh gi√° {n_clusters} c·ª•m...")
            
            eval_result = self._evaluate_single_config(X, n_clusters)
            evaluations.append(eval_result)
            
            self.logger.info(
                f"   ‚úÖ Silhouette: {eval_result.silhouette_score:.4f}, "
                f"Distance: {eval_result.total_distance:.4f}, "
                f"Time: {eval_result.training_time:.2f}s"
            )
        
        # Ch·ªçn c·∫•u h√¨nh t·ªët nh·∫•t
        if selection_metric == "silhouette":
            best_eval = max(evaluations, key=lambda x: x.silhouette_score)
        elif selection_metric == "distance":
            best_eval = min(evaluations, key=lambda x: x.total_distance)
        elif selection_metric == "elbow":
            best_eval = self._find_elbow(evaluations)
        else:
            best_eval = max(evaluations, key=lambda x: x.silhouette_score)
        
        total_time = time.time() - start_time
        
        result = AutoClusterResult(
            best_n_clusters=best_eval.n_clusters,
            best_evaluation=best_eval,
            all_evaluations=evaluations,
            total_time=total_time
        )
        
        self.logger.info("\n" + result.summary())
        
        return result
    
    def _find_elbow(self, evaluations: List[ClusterEvaluation]) -> ClusterEvaluation:
        """
        T√¨m ƒëi·ªÉm elbow trong ƒë·ªì th·ªã distance.
        
        S·ª≠ d·ª•ng ph∆∞∆°ng ph√°p t√≠nh g√≥c ƒë·ªÉ t√¨m ƒëi·ªÉm u·ªën.
        """
        n_points = len(evaluations)
        if n_points < 3:
            return evaluations[0]
        
        # L·∫•y distances
        distances = np.array([e.total_distance for e in evaluations])
        x = np.arange(n_points)
        
        # Normalize
        x_norm = x / x.max()
        d_norm = (distances - distances.min()) / (distances.max() - distances.min() + 1e-10)
        
        # T√≠nh kho·∫£ng c√°ch t·ª´ m·ªói ƒëi·ªÉm ƒë·∫øn ƒë∆∞·ªùng th·∫≥ng n·ªëi 2 ƒë·∫ßu
        line_vec = np.array([x_norm[-1] - x_norm[0], d_norm[-1] - d_norm[0]])
        line_vec = line_vec / np.linalg.norm(line_vec)
        
        distances_to_line = []
        for i in range(n_points):
            point_vec = np.array([x_norm[i] - x_norm[0], d_norm[i] - d_norm[0]])
            # Kho·∫£ng c√°ch vu√¥ng g√≥c ƒë·∫øn ƒë∆∞·ªùng th·∫≥ng
            dist = np.abs(np.cross(line_vec, point_vec))
            distances_to_line.append(dist)
        
        elbow_idx = np.argmax(distances_to_line)
        return evaluations[elbow_idx]
    
    def run_with_batches(
        self,
        X: np.ndarray,
        batch_size: int = 50000,
        selection_metric: str = "silhouette"
    ) -> AutoClusterResult:
        """
        Ch·∫°y auto clustering v·ªõi batch processing.
        
        D√πng cho dataset l·ªõn ƒë·ªÉ tr√°nh tr√†n RAM.
        
        Args:
            X: Features array
            batch_size: K√≠ch th∆∞·ªõc m·ªói batch
            selection_metric: Metric ƒë·ªÉ ch·ªçn c·ª•m
        
        Returns:
            AutoClusterResult
        """
        n_samples = X.shape[0]
        
        if n_samples <= batch_size:
            return self.run(X, selection_metric)
        
        self.logger.info(f"üì¶ Dataset l·ªõn ({n_samples:,} samples), s·ª≠ d·ª•ng sampling")
        
        # Sample d·ªØ li·ªáu ƒë·ªÉ t√¨m c·∫•u h√¨nh t·ªët nh·∫•t
        sample_size = min(batch_size, n_samples)
        indices = np.random.choice(n_samples, sample_size, replace=False)
        X_sample = X[indices]
        
        return self.run(X_sample, selection_metric)


def auto_cluster(
    X: np.ndarray,
    min_clusters: int = 2,
    max_clusters: int = 9,
    optimize: bool = True,
    max_memory_gb: float = 4.0,
    center_init: str = "ver6"
) -> AutoClusterResult:
    """
    H√†m ti·ªán √≠ch ƒë·ªÉ ch·∫°y auto clustering nhanh.
    
    Args:
        X: Features array
        min_clusters: S·ªë c·ª•m t·ªëi thi·ªÉu
        max_clusters: S·ªë c·ª•m t·ªëi ƒëa
        optimize: C√≥ t·ªëi ∆∞u params kh√¥ng
        max_memory_gb: RAM t·ªëi ƒëa
    
    Returns:
        AutoClusterResult
    
    Example:
        >>> result = auto_cluster(X, min_clusters=2, max_clusters=9)
        >>> print(f"Best: {result.best_n_clusters} clusters")
    """
    pipeline = AutoClusterPipeline(
        min_clusters=min_clusters,
        max_clusters=max_clusters,
        optimize_params=optimize,
        max_memory_gb=max_memory_gb,
        log_level="INFO",
        center_init=center_init
    )
    return pipeline.run(X)

