# run_experiment.py
import argparse
import sys
import numpy as np
from pathlib import Path

# ƒê·∫£m b·∫£o python t√¨m th·∫•y src
sys.path.append(str(Path(__file__).parent))

from src.auto_cluster import AutoClusterPipeline
from src.data_loader import DataLoader
from src.classifier import ClusterClassifier
from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.base import BaseEstimator
import os

def main():
    parser = argparse.ArgumentParser(description="HAC Auto-Cluster Experiment Runner")
    
    # 1. Dataset & Mode
    # [STYLE] B·ªè path m·∫∑c ƒë·ªãnh hardcode ƒë·ªÉ tr√°nh l·ªói tr√™n m√°y kh√°c
    parser.add_argument("--file", type=str, default="/home/nhannv/Hello/ICN/data_process/dataset_5percent_33classes_no_normalize.csv", help="Path to input CSV")
    parser.add_argument("--label", type=str, default="label", help="Label column name")
    parser.add_argument("--init", type=str, default="ver6", choices=["ver6", "legacy"], help="Center init mode")
    
    # 2. Search Range (Internal AutoCluster t√¨m K t·ªët nh·∫•t trong kho·∫£ng n√†y)
    parser.add_argument("--min_k", type=int, default=2)
    parser.add_argument("--max_k", type=int, default=10)
    
    # 3. Environment Config
    parser.add_argument("--use_ig", action="store_true", help="Enable Information Gain Feature Selection")
    parser.add_argument("--ig_k", type=int, default=40, help="Number of features to keep if IG is on")
    parser.add_argument("--norm", type=str, default="minmax", choices=["minmax", "zscore"], help="Normalization method")
    
    # 4. Classification Model
    parser.add_argument("--classifier", type=str, default="gb", 
                        choices=["gb", "rf", "dt", "svc", "lr"],
                        help="Classification model: gb=GradientBoosting, rf=RandomForest, dt=DecisionTree, svc=SVC, lr=LogisticRegression")
    
    # 5. System
    parser.add_argument("--max_mem", type=float, default=4.0, help="Max memory in GB")
    parser.add_argument("--n_jobs", type=int, default=-1, help="Number of parallel jobs (-1 = all CPUs, 1 = sequential)")

    args = parser.parse_args()

    # X√°c ƒë·ªãnh s·ªë CPU cores ƒë·ªÉ s·ª≠ d·ª•ng
    n_cpus = os.cpu_count() or 1
    n_jobs_actual = n_cpus if args.n_jobs == -1 else max(1, args.n_jobs)
    
    # Map classifier name to model (s·ª≠ d·ª•ng tham s·ªë m·∫∑c ƒë·ªãnh c·ªßa sklearn + n_jobs)
    classifier_map = {
        "gb": ("GradientBoosting", GradientBoostingClassifier(random_state=42)),  # GB kh√¥ng c√≥ n_jobs
        "rf": ("RandomForest", RandomForestClassifier(random_state=42, n_jobs=n_jobs_actual)),
        "dt": ("DecisionTree", DecisionTreeClassifier(random_state=42)),  # DT kh√¥ng c√≥ n_jobs
        "svc": ("SVC", SVC(random_state=42, probability=True)),  # SVC kh√¥ng c√≥ n_jobs
        "lr": ("LogisticRegression", LogisticRegression(random_state=42, n_jobs=n_jobs_actual))
    }
    classifier_name, base_classifier = classifier_map[args.classifier]
    
    print("\n" + "="*60)
    print(f"üß™ EXPERIMENT START")
    print(f"üìÇ File:      {args.file}")
    print(f"‚öôÔ∏è  Mode:      {args.init.upper()} (Range K={args.min_k}..{args.max_k})")
    print(f"üîç Feature:   {'IG Enabled (Top ' + str(args.ig_k) + ')' if args.use_ig else 'Original Features'}")
    print(f"üìè Norm:      {args.norm} (tr∆∞·ªõc khi split)")
    print(f"ü§ñ Classifier: {classifier_name}")
    print(f"üíª CPUs:       {n_cpus} cores (using {n_jobs_actual} jobs)")
    print("="*60 + "\n")
    
    try:
        # --- B∆Ø·ªöC 1: Load d·ªØ li·ªáu ---
        # [NEW] ƒê·ªïi logic: Normalize TR∆Ø·ªöC ‚Üí Split SAU (c·∫£ 2 lu·ªìng ƒë·ªÅu nh∆∞ v·∫≠y)
        print("‚è≥ Loading & Preprocessing data...")
        
        # S·ª≠ d·ª•ng DataLoader cho c·∫£ 2 tr∆∞·ªùng h·ª£p ƒë·ªÉ ƒë·∫£m b·∫£o consistency
        # Logic: Load ‚Üí Remove constant ‚Üí Normalize ‚Üí Calculate IG ‚Üí Split
        loader = DataLoader(log_level="INFO")
        X_train, X_test, y_train, y_test, ig_weights = loader.load_and_preprocess(
            file_path=args.file,
            label_column=args.label,
            normalize_method=args.norm,
            remove_constant=True,
            calculate_ig=args.use_ig,
            test_size=0.2,
            random_state=42
        )
        
        # V√¨ ƒë√¢y l√† experiment t√¨m s·ªë c·ª•m (Unsupervised), ta n√™n g·ªôp l·∫°i ƒë·ªÉ ch·∫°y tr√™n to√†n b·ªô d·ªØ li·ªáu
        X_full = np.concatenate((X_train, X_test), axis=0)
        
        # √Åp d·ª•ng IG weights n·∫øu c√≥
        if args.use_ig and ig_weights is not None:
             # L·∫•y top K features t·ªët nh·∫•t d·ª±a tr√™n weights (logic ƒë∆°n gi·∫£n h√≥a)
             # Ho·∫∑c nh√¢n weights v√†o X nh∆∞ trong logic clustering c≈©
             # ·ªû ƒë√¢y ta nh√¢n weights tr·ª±c ti·∫øp ƒë·ªÉ AutoCluster d√πng
             # Gh√©p weight c·ªßa train/test l·∫°i (l∆∞u √Ω ig_weights tr·∫£ v·ªÅ vector features, d√πng chung cho c·∫£ t·∫≠p)
             X_full = X_full * ig_weights
             X_train = X_train * ig_weights  # [NEW] C≈©ng √°p d·ª•ng cho train/test
             X_test = X_test * ig_weights
             print(f"‚úÖ Applied Information Gain weights")

        print(f"‚úÖ Data ready: Train={X_train.shape[0]}, Test={X_test.shape[0]}, Features={X_train.shape[1]}")

        # --- B∆Ø·ªöC 2: Kh·ªüi t·∫°o Auto Pipeline ---
        auto = AutoClusterPipeline(
            min_clusters=args.min_k,
            max_clusters=args.max_k,
            center_init=args.init,
            optimize_params=True, # Lu√¥n t·ªëi ∆∞u theta/alpha
            max_memory_gb=args.max_mem,
            n_jobs=n_jobs_actual,  # [NEW] Th√™m ƒëa lu·ªìng
            log_level="INFO"
        )

        # --- B∆Ø·ªöC 3: Ch·∫°y Experiment ---
        # [FIX] Truy·ªÅn numpy array X v√†o thay v√¨ ƒë∆∞·ªùng d·∫´n file
        # Result tr·∫£ v·ªÅ l√† object AutoClusterResult, KH√îNG ph·∫£i tuple
        result = auto.run(X_full, selection_metric="silhouette")

        # --- B∆Ø·ªöC 4: Hi·ªÉn th·ªã k·∫øt qu·∫£ clustering ---
        print("\n" + "="*60)
        print(f"üèÜ CLUSTERING RESULT")
        print(f"   Best K:         {result.best_n_clusters}")
        # Truy c·∫≠p v√†o best_evaluation ƒë·ªÉ l·∫•y metrics
        print(f"   Best Silhouette:{result.best_evaluation.silhouette_score:.4f}")
        print(f"   Best Params:    Œ∏={result.best_evaluation.theta:.4f}, Œ±={result.best_evaluation.alpha:.4f}")
        print(f"   Total Time:     {result.total_time:.2f}s")
        print("="*60 + "\n")
        
        # In b·∫£ng chi ti·∫øt t·ª´ ph∆∞∆°ng th·ª©c c√≥ s·∫µn
        print(result.summary())
        
        # --- B∆Ø·ªöC 5: Training v√† ƒë√°nh gi√° model classification ---
        print("\n" + "="*60)
        print("ü§ñ TRAINING CLASSIFICATION MODEL")
        print("="*60)
        
        # Kh·ªüi t·∫°o classifier v·ªõi s·ªë c·ª•m t·ªët nh·∫•t
        classifier = ClusterClassifier(
            n_clusters=result.best_n_clusters,
            theta=result.best_evaluation.theta,
            alpha=result.best_evaluation.alpha,
            center_init=args.init,
            base_classifier=base_classifier,  # [NEW] S·ª≠ d·ª•ng model ƒë∆∞·ª£c ch·ªçn
            log_level="INFO"
        )
        
        # Train model
        print(f"‚è≥ Training model v·ªõi {result.best_n_clusters} c·ª•m...")
        classifier.fit(
            X_train, 
            y_train,
            information_gain_weights=ig_weights if args.use_ig else None
        )
        training_time = classifier.training_time
        print(f"‚úÖ Training completed in {training_time:.2f}s")
        
        # Test v√† ƒë√°nh gi√°
        print(f"‚è≥ Testing model...")
        prediction_result = classifier.predict(X_test, y_test)
        
        # --- B∆Ø·ªöC 6: Hi·ªÉn th·ªã k·∫øt qu·∫£ classification ---
        print("\n" + "="*60)
        print(f"üìä CLASSIFICATION RESULTS")
        print("="*60)
        print(f"   Accuracy:       {prediction_result.accuracy:.4f} ({prediction_result.accuracy*100:.2f}%)")
        print(f"   Precision:      {prediction_result.precision:.4f}")
        print(f"   Recall:         {prediction_result.recall:.4f}")
        print(f"   F1-Score:       {prediction_result.f1:.4f}")
        print(f"   Training Time:  {training_time:.2f}s")
        print(f"   Testing Time:   {prediction_result.total_time:.4f}s")
        print("="*60)
        
        # In classification report chi ti·∫øt
        print("\nüìã Classification Report:")
        print(prediction_result.classification_report)
        
        # T√≥m t·∫Øt cu·ªëi c√πng
        print("\n" + "="*60)
        print("üéØ FINAL SUMMARY")
        print("="*60)
        print(f"   Classifier:     {classifier_name}")
        print(f"   Best Clusters:  {result.best_n_clusters}")
        print(f"   Best Params:    Œ∏={result.best_evaluation.theta:.4f}, Œ±={result.best_evaluation.alpha:.4f}")
        print(f"   Silhouette:     {result.best_evaluation.silhouette_score:.4f}")
        print(f"   Accuracy:       {prediction_result.accuracy:.4f} ({prediction_result.accuracy*100:.2f}%)")
        print(f"   F1-Score:       {prediction_result.f1:.4f}")
        print(f"   Total Time:     {result.total_time + training_time + prediction_result.total_time:.2f}s")
        print("="*60 + "\n")
        
    except Exception as e:
        print(f"\n‚ùå CRITICAL ERROR: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()